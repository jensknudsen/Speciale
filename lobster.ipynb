{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How to run much code up in the sky \n",
    "2. How to acess the data from the csv-file "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Lobster Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lobster data is an online limit order book data tool. This is a tool that gives access to the entire universe of NASDAQ traded stocks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lobster database offers the NASDAQ's historical TotalView-ITCH files. This is data that provide detailed information about every trade and quote for all NASDAQ-listed stocks. In the interface it is possible customize the level of detail for the data e.g. the level. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Submissions, cancellations and executions (visible and hidden) are seperately identified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each limit order event the following information is provided: Time stamp(up to nanosecond precision), order ID, price, size and buy/sell indicator.\n",
    "The database contains data from the 27th of june 2007 up to the day before yesterday. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Structure "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lobster generates two files - 'message' and 'orderbook' file for each active trading day:\n",
    "1. The 'orderbook' file contains the evolution of the limit order book up to the desirede number of levels. \n",
    "2. The 'message' file contains indicators for the type of event causing an update of the limit order book in the requested price range. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All events are timestamped to second after midnight with decimal precision up to nanoseconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the following: \n",
    "1. Time (ts) seconds after midnight with up to nanoseconds precision\n",
    "2. Event type:(1: Submission of a new limit order)(2: Cancellation (partial deletion of a limit order))(3: Deletion (total deletion of a limit order))(4: Execution of a visible limit order)(5: Execution of a hidden limit order)(6: Indicates a cross trade, e.g. auction trade)(7: Trading halt indicator (detailed information below))\n",
    "3. Order ID: Unique order reference number\n",
    "4. Size: Number of shares\n",
    "5. Price: Dollar price times 10000\n",
    "6. Direction: (-1: sell limit order) (1:Buy limit order) (note: Execution of a sell (buy) limit order corresponds to a buyer (seller) initiated trade, i.e. buy (sell) trade.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Book File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains ask and bid prices and ask and bid sizes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following variables:\n",
    "1. Ask Price 1: Level 1 ask price (best ask price)\n",
    "2. Ask Size 1: Level 1 ask volume (best ask volume)\n",
    "3. Bid Price 1: Level 1 bid price (best bid price)\n",
    "4. Bid Size 1: Level 1 bid volume (best bid volume)\n",
    "5. Ask Price 2: Level 2 ask price (second best ask price)\n",
    "6. Ask Size 2: Level 2 ask volume (second best ask volume) \n",
    "7. ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the message file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General - how does it work?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limit order book data is contracted based on the NASDAQ's histotical TotalView-ITCH files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of streaming the state of the entire limit book order after each update, the Lobster data only display the information when the limit order event changes the order book. Here will each limit order submission, cancellation and execution result in an individual event message being streamed to the market."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Compare, for example, a level 1 and a level 25 request. In case of a level 1 request only 'trades and quotes', i.e. changes to the best bid and ask prices and their respective volumes are recorded. The limit order book saved to the output file in case of a level 25 request is updated every time a price or volume changes in the range from the 25th best bid to the 25th best ask price.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About order book modeling (https://towardsdatascience.com/application-of-gradient-boosting-in-order-book-modeling-3cd5f71575a7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order book is a electronic list of buy and sell orders for a specific security or finansial instrument. Here the orderbook lists the amount being bid or offered for each price point. \n",
    "N.B. -> Market depth can help traders determine the direction of future price movements. (Here can the market depth be able to tell if the market is able to absorb large market order without significantly impacting the price).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mid-price is the price between the best price of the seller and the the best price of the buyer (in our data the midprice will be: (ask_price_1+bid_price_1)/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is generated with inspiration from steffan voigt (https://www.voigtstefan.me/post/lobster-1/)\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_orderbook_data(assets, date, level, directory):\n",
    "    \"\"\"\n",
    "    Processes order book data for a given set of assets on a specific date.\n",
    "\n",
    "    This function reads message and order book data from CSV files for each \n",
    "    asset, processes the data by modifying timestamps, prices, and computing \n",
    "    additional metrics like midquote, spread, and volume. Finally, it saves the \n",
    "    processed data into Feather format files.\n",
    "\n",
    "    Parameters:\n",
    "    assets (list): A list of asset symbols (e.g., ['AAPL', 'TSLA']).\n",
    "    date (str): The date for which the data is processed, in 'YYYY-MM-DD' format.\n",
    "    level (int): The level of the order book data to process.\n",
    "    directory (str): The directory path where the data files are located and where \n",
    "                     the processed files will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Change the working directory to the specified directory:\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Iterate over each asset in the list:\n",
    "    for dates in date:\n",
    "        for asset in assets:\n",
    "            # Construct file names for messages and order book data:\n",
    "            messages_filename = f\"{asset}_{dates}_34200000_57600000_message_{level}.csv\"\n",
    "            orderbook_filename = f\"{asset}_{dates}_34200000_57600000_orderbook_{level}.csv\"\n",
    "\n",
    "            # Read messages data from CSV:\n",
    "            messages_raw = pd.read_csv(\n",
    "                messages_filename, \n",
    "                names=[\"ts\", \"type\", \"order_id\", \"m_size\", \"m_price\", \"direction\", \"null\"],\n",
    "                dtype={\"ts\": float, \"type\": int, \"order_id\": int, \"m_size\": float, \"m_price\": float, \"direction\": int, \"null\": object}\n",
    "            )\n",
    "\n",
    "            # Convert timestamp to datetime and adjust for time zone and date:\n",
    "            messages_raw[\"ts\"] = messages_raw[\"ts\"].apply(\n",
    "                lambda x: datetime.fromtimestamp(x, tz=pytz.timezone('GMT')).replace(tzinfo=pytz.timezone('GMT')).replace(year=int(dates[0:4]), month=int(dates[5:7]), day=int(dates[8:10]))\n",
    "            )\n",
    "\n",
    "            # Adjust message prices:\n",
    "            messages_raw[\"m_price\"] = messages_raw[\"m_price\"] / 10000\n",
    "\n",
    "            # Read order book data from CSV:\n",
    "            orderbook_raw = pd.read_csv(\n",
    "                orderbook_filename, \n",
    "                header=None,\n",
    "                names=[f\"{col}_{i+1}\" for i in range(level) for col in [\"ask_price\", \"ask_size\", \"bid_price\", \"bid_size\"]],\n",
    "                dtype=np.float64\n",
    "            )\n",
    "\n",
    "            # Adjust order book prices:\n",
    "            orderbook_raw = orderbook_raw.apply(lambda x: x/10000 if \"price\" in x.name else x)\n",
    "\n",
    "            # Merge messages and order book data:\n",
    "            orderbook = pd.concat([messages_raw, orderbook_raw], axis=1)\n",
    "            del orderbook[\"null\"]  # delete the unnecessary 'null' column\n",
    "\n",
    "            # Calculate midquote, spread, and volume:\n",
    "            orderbook[\"midquote\"] = orderbook.apply(lambda x: (x[\"ask_price_1\"]/2) + (x[\"bid_price_1\"]/2), axis=1)\n",
    "            orderbook[\"spread\"] = orderbook.apply(lambda x: ((x[\"ask_price_1\"] - x[\"bid_price_1\"])/x[\"midquote\"]) * 10000, axis=1)\n",
    "            orderbook[\"volume\"] = np.where((orderbook[\"type\"] == 4) | (orderbook[\"type\"] == 5), orderbook[\"m_size\"], 0)\n",
    "            #orderbook[\"hidden_volume\"] = np.where(orderbook[\"type\"] == 5, orderbook<[\"m_size\"], 0)\n",
    "\n",
    "            # Add a column with the asset name:\n",
    "            orderbook[\"asset\"] = f\"{asset}\"\n",
    "\n",
    "            # Save the processed data to a Feather file:\n",
    "            orderbook.to_feather(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/Feather/{asset}_{dates}_orderbook_{level}.feather\")\n",
    "            print('Done with asset: ', asset)\n",
    "        print('Done with date: ', dates)\n",
    "\n",
    "# Usage example:\n",
    "asset = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "date = [\"2023-03-21\",\"2023-03-22\",\"2023-03-23\"]\n",
    "level = 15\n",
    "directory = r\"/Users/jensknudsen/Desktop/LOBSTER_DATA/Data\"\n",
    "process_orderbook_data(asset, date, level, directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK MAX DEPTH ALLOWED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to detect the lowest value in the orderbook\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def concatenate_orderbooks(asset_list, date, level, directory):\n",
    "    orderbook_df_list = []\n",
    "    for asset in asset_list:\n",
    "        file_path = os.path.join(directory, f'{asset}_{date}_orderbook_{level}.feather')\n",
    "        orderbook_df_list.append(pd.read_feather(file_path))\n",
    "    orderbook = pd.concat(orderbook_df_list)\n",
    "    return orderbook\n",
    "\n",
    "\n",
    "# df_returns = pd.DataFrame()\n",
    "\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "\n",
    "# ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'APO', 'ARM', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BRK.B', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CB', 'CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'E', 'ECL', 'EL', 'ELV', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FI', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITUB', 'ITW', 'JNJ', 'JPM', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'MUFG', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PBR', 'PBR.A', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RTX', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPGI', 'STLA', 'SYK', 'T', 'TD', 'TDG', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "\n",
    "date = [\"2023-03-21\",\"2023-03-22\",\"2023-03-23\"]\n",
    "level = 15\n",
    "prediction_ahead = 4\n",
    "time_interval = '5S' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek;500L=0,5 sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "depth = 5 # also 10, 30, 50\n",
    "window = 5 # also 3, 5, 10\n",
    "directory = r\"/Users/jensknudsen/Desktop/LOBSTER_DATA/Feather\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "minvalue_list =[]\n",
    "\n",
    "def find_column_with_value(df, value):\n",
    "    \"\"\"\n",
    "    Find the column name in a pandas DataFrame that contains a specific float64 value.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to search.\n",
    "    value (float): The float64 value to search for.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of column names that contain the specified value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        columns_with_value = []\n",
    "\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'float64' and (df[column] == value).any():\n",
    "                columns_with_value.append(column)\n",
    "        return_value = columns_with_value[0]\n",
    "        # extract only the number as a interger with regax\n",
    "        return_value = int(''.join(filter(str.isdigit, return_value)))\n",
    "\n",
    "        return return_value\n",
    "    except:\n",
    "        return 15\n",
    "\n",
    "minvalue_list =[]\n",
    "\n",
    "for e in date:\n",
    "    for i in asset_list:\n",
    "        df_test = concatenate_orderbooks([i], e, level, directory)\n",
    "        # Example usage:\n",
    "        a = find_column_with_value(df_test, 199999)\n",
    "        b = find_column_with_value(df_test, -999999.9999)\n",
    "        c = find_column_with_value(df_test, 999999.9999)\n",
    "\n",
    "        # return the lowest value of the three\n",
    "        min_value = min(a, b, c)\n",
    "        minvalue_list.append(min_value)\n",
    "\n",
    "minvalueoverall = min(minvalue_list)\n",
    "print(f\"This is the maximum number of depth that is allowed in this timehorizon is: {minvalueoverall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CNI', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TM', 'TMO', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UPS', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'ZTS']\n",
    "len(asset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE WITH THE RIGHT PICTURES\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# BMO + CAT + CNQ + FDX + GS + TJX +TRV +UNP+V +WMT +XOM = done in 0,1 sek\n",
    "# ADSK + APH + CMG + ECL + ROP= 0,01 sek\n",
    "\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CNI', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TM', 'TMO', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UPS', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'ZTS']#['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TM', 'TMO', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UPS', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'ZTS'] # ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']  #['AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AEP', 'ALGN', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'ANSS', 'ASML', 'AVGO', 'AZN', 'BIIB', 'BKNG', 'BKR', 'CDNS', 'CEG', 'CHTR', 'CMCSA', 'COST', 'CPRT', 'CRWD', 'CSCO', 'CSGP', 'CSX', 'CTAS', 'CTSH', 'DDOG', 'DLTR', 'DXCM', 'EA', 'EBAY', 'ENPH', 'EXC', 'FANG', 'FAST', 'FTNT', 'GEHC', 'GFS', 'GILD', 'GOOGL', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'JD', 'KDP', 'KHC', 'KLAC', 'LCID', 'LRCX', 'LULU', 'MAR', 'MDLZ', 'MELI', 'META', 'MNST', 'MRNA', 'MRVL', 'MSFT', 'MU', 'NFLX', 'NVDA', 'NXPI', 'ODFL', 'ON', 'ORLY', 'PANW', 'PAYX', 'PCAR', 'PDD', 'PEP', 'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SGEN', 'SIRI', 'SNPS', 'TEAM', 'TMUS', 'TSLA', 'TXN', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZM', 'ZS'] # ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'APO', 'ARM', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BRK.B', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CB', 'CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'E', 'ECL', 'EL', 'ELV', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FI', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITUB', 'ITW', 'JNJ', 'JPM', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'MUFG', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PBR', 'PBR.A', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RTX', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPGI', 'STLA', 'SYK', 'T', 'TD', 'TDG', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "\n",
    "#original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")\n",
    "\n",
    "level = 15\n",
    "prediction_ahead = 4 # This the the duration of each picture 4*time_interval \n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '10L' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 500L=0,5sek, 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10\n",
    "date_listen =[\"2023-03-21\"]#[\"2023-03-21\",\"2023-03-22\",\"2023-03-23\"]\n",
    "\n",
    "\n",
    "def create_folder(depth, time_interval, window):\n",
    "    \"\"\"\n",
    "    Create new folders with a specified naming convention at two different locations.\n",
    "    If the folders already exist, prompt the user for permission to delete existing files.\n",
    "\n",
    "    Parameters:\n",
    "    depth (int): Depth parameter used in the folder names.\n",
    "    time_interval (int): Time interval parameter used in the folder names.\n",
    "    window (int): Window parameter used in the folder names.\n",
    "    \"\"\"\n",
    "    for dato in date_listen:\n",
    "        # First folder path\n",
    "        folder_path_1 = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}_date{dato}'\n",
    "\n",
    "        # Second folder path\n",
    "        folder_path_2 = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/Returns/depth{depth}_time{time_interval}_window{window}_date{dato}'\n",
    "\n",
    "        # Function to create or clear a folder\n",
    "        def create_or_clear_path(folder_path):\n",
    "            if os.path.exists(folder_path):\n",
    "                response = input(f\"Folder already exists: {folder_path}. Delete files in it? (yes/no): \").strip().lower()\n",
    "                if response == 'yes':\n",
    "                    shutil.rmtree(folder_path)\n",
    "                    os.makedirs(folder_path, exist_ok=True)\n",
    "                    print(f\"Folder cleared and recreated: {folder_path}\")\n",
    "                else:\n",
    "                    print(f\"Folder retained with existing files: {folder_path}\")\n",
    "            else:\n",
    "                os.makedirs(folder_path, exist_ok=True)\n",
    "                print(f\"Folder created: {folder_path}\")\n",
    "\n",
    "        # Create or clear the first folder\n",
    "        create_or_clear_path(folder_path_1)\n",
    "\n",
    "        # Create or clear the second folder\n",
    "        create_or_clear_path(folder_path_2)\n",
    "\n",
    "# Example usage\n",
    "create_folder(depth,time_interval, window)\n",
    "\n",
    "\n",
    "for w in date_listen:\n",
    "    original_starttime = datetime.fromisoformat(f\"{w} 09:30:00.000000+00:00\")  \n",
    "    for i in asset_list:\n",
    "        datetime_obj = datetime.fromisoformat(f\"{w} 09:33:00.000+00:00\") # default is 16:00:00.000+00:00\n",
    "        datetime_obj = datetime_obj - pd.to_timedelta(window * prediction_ahead+1) # to avoid problem in the end of the prediction horizon (due to the return)\n",
    "        a = original_starttime\n",
    "        b = datetime.fromisoformat(f\"{w} 09:31:05.000000+00:00\")\n",
    "        c = original_starttime\n",
    "        while c < datetime_obj:\n",
    "            # Helper function to concatenate orderbooks\n",
    "            def concatenate_orderbooks(asset_list, date, level, directory):\n",
    "                orderbook_df_list = []\n",
    "                for asset in asset_list:\n",
    "                    file_path = os.path.join(directory, f'{asset}_{date}_orderbook_{level}.feather')\n",
    "                    orderbook_df_list.append(pd.read_feather(file_path))\n",
    "                orderbook = pd.concat(orderbook_df_list)\n",
    "                return orderbook\n",
    "\n",
    "            def round_up_timestamp(ts, freq):\n",
    "                \"\"\"Round up a timestamp based on a given frequency\"\"\"\n",
    "                return (ts + pd.Timedelta(freq) - pd.Timedelta('1ns')).floor(freq)\n",
    "\n",
    "            def split_by_seconds_optimized(df, freq, window):\n",
    "                df['ts'] = pd.to_datetime(df['ts'])\n",
    "                # Round up the timestamps\n",
    "                df['ts'] = df['ts'].apply(lambda x: round_up_timestamp(x, freq))\n",
    "                df.set_index('ts', inplace=True)\n",
    "                if 'asset' not in df.columns:\n",
    "                    raise ValueError(\"DataFrame must have an 'asset' column\")\n",
    "                asset_results = {}\n",
    "                for asset, asset_df in df.groupby('asset'):\n",
    "                    # Resample the dataframe\n",
    "                    resampled_df = asset_df.resample(freq).last()\n",
    "                    # Forward-fill any NaN values\n",
    "                    resampled_df.ffill(inplace=True)\n",
    "                    # If the first row(s) are still NaN, drop them\n",
    "                    resampled_df.dropna(inplace=True)\n",
    "                    interval_dataframes = resampled_df.groupby(resampled_df.index).tail(1)\n",
    "                    cols_to_drop = [\"type\", \"order_id\", \"m_size\", \"m_price\",\"spread\", \"direction\", \"volume\", \"midquote\", \"asset\"] # \"hidden_volume\"\n",
    "                    interval_dataframes.drop(columns=cols_to_drop, inplace=True)\n",
    "                    interval_dataframes = interval_dataframes.T\n",
    "                    rolling_windows = [None] * (len(interval_dataframes.columns) - (window - 1))\n",
    "                    for start_col in range(len(interval_dataframes.columns) - (window - 1)):\n",
    "                        end_col = start_col + window\n",
    "                        window_df = interval_dataframes.iloc[:, start_col:end_col]\n",
    "                        rolling_windows[start_col] = window_df\n",
    "                    asset_results[asset] = rolling_windows\n",
    "                return asset_results\n",
    "\n",
    "            # Function to truncate the results based on x\n",
    "            def truncate_results(asset_results, depth):\n",
    "                truncated_results = {}\n",
    "                num_rows = depth * 4  # Convert depth to number of rows to keep\n",
    "                for asset, windows in asset_results.items():\n",
    "                    truncated_windows = [window.head(num_rows) for window in windows]\n",
    "                    truncated_results[asset] = truncated_windows\n",
    "                return truncated_results\n",
    "\n",
    "            # Main function to process orderbooks\n",
    "            def process_orderbooks(asset_list, w, level, freq, window, start_time, end_time, directory, depth):\n",
    "                orderbook = concatenate_orderbooks(asset_list, w, level, directory)\n",
    "                orderbook = orderbook.loc[orderbook['ts'].between(start_time, end_time)].reset_index(drop=True)\n",
    "                asset_results = split_by_seconds_optimized(orderbook, freq, window)\n",
    "                asset_results = truncate_results(asset_results, depth)\n",
    "                return asset_results\n",
    "\n",
    "            # USE:\n",
    "            asset = [i]\n",
    "            date = f\"{w}\"\n",
    "            directory = r\"/Users/jensknudsen/Desktop/LOBSTER_DATA/Feather\"\n",
    "            start_time = a #\"2023-03-21 09:30:19.975000+00:00\"\n",
    "            end_time = b\n",
    "            #Dinamic parameters:\n",
    "            #prediction_ahead = 3\n",
    "            #depth = 10 # also 10, 30, 50\n",
    "            #time_interval = '30S' #: 10S = 10sek, 1S=1sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "            #window = 3 # also 3, 5, 10\n",
    "\n",
    "            results = process_orderbooks(asset, date, level, time_interval, window, start_time, end_time, directory, depth)\n",
    "            # make the a copy of the data results to be asset_results_10ms\n",
    "\n",
    "            asset_results_10ms = results.copy()\n",
    "            \n",
    "            # NEW IMPROVE CODE\n",
    "            from helper_functions import scale_dataframe\n",
    "            from concurrent.futures import ProcessPoolExecutor\n",
    "            import os\n",
    "\n",
    "            def scale_dataframes_concurrently(dfs):\n",
    "                # Using ProcessPoolExecutor to parallelize the scaling\n",
    "                # Using os.cpu_count() to get the number of available CPU cores\n",
    "                num_workers = os.cpu_count() - 1  # Using all cores minus one\n",
    "                with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "                    return list(executor.map(scale_dataframe, dfs))\n",
    "\n",
    "            def scale_asset_dataframes(asset_data):\n",
    "                for asset, dfs in asset_data.items():\n",
    "                    # Use the concurrent scaling function\n",
    "                    asset_data[asset] = scale_dataframes_concurrently(dfs)\n",
    "                return asset_data\n",
    "\n",
    "            from datetime import datetime, timedelta\n",
    "\n",
    "            def add_time_units(original_starttime, unit, units_to_add):\n",
    "                # Define the conversion of each unit to seconds\n",
    "                unit_conversion = {\n",
    "                    '60S': 60,         # 60 seconds\n",
    "                    '30S': 30,         # 30 seconds\n",
    "                    '10S': 10,         # 10 second\n",
    "                    '5S': 5,         # 5 second\n",
    "                    '1S': 1,         # 1 second\n",
    "                    '500L':0.5,      # 0.5 second   \n",
    "                    '100L': 0.1,     # 0.1 seconds\n",
    "                    '10L': 0.01,     # 0.01 seconds\n",
    "                    '1L': 0.001,     # 0.001 seconds\n",
    "                    '100U': 0.001    # 0.001 seconds\n",
    "                }\n",
    "\n",
    "                # Calculate the total seconds to add\n",
    "                total_seconds_to_add = units_to_add * unit_conversion[unit]\n",
    "\n",
    "                # Add the time to the original datetime\n",
    "                return original_starttime + timedelta(seconds=total_seconds_to_add)\n",
    "\n",
    "            def minus_time_units(original_starttime, unit, units_to_add):\n",
    "                # Define the conversion of each unit to seconds\n",
    "                unit_conversion = {\n",
    "                    '60S': 60,         # 60 seconds\n",
    "                    '30S': 30,         # 30 seconds\n",
    "                    '10S': 10,         # 10 second\n",
    "                    '5S': 5,         # 5 second\n",
    "                    '1S': 1,         # 1 second\n",
    "                    '500L':0.5,      # 0.5 second\n",
    "                    '100L': 0.1,     # 0.1 seconds\n",
    "                    '10L': 0.01,     # 0.01 seconds\n",
    "                    '1L': 0.001,     # 0.001 seconds\n",
    "                    '100U': 0.001    # 0.001 seconds\n",
    "                }\n",
    "\n",
    "                # Calculate the total seconds to add\n",
    "                total_seconds_to_add = units_to_add * unit_conversion[unit]\n",
    "\n",
    "                # Add the time to the original datetime\n",
    "                return original_starttime - timedelta(seconds=total_seconds_to_add)\n",
    "\n",
    "            def sub_time_units(unit, units_to_add):\n",
    "                # Define the conversion of each unit to seconds\n",
    "                unit_conversion = {\n",
    "                    '60S': 60,         # 60 seconds\n",
    "                    '30S': 30,         # 30 seconds\n",
    "                    '10S': 10,         # 10 second\n",
    "                    '5S': 5,         # 5 second\n",
    "                    '1S': 1,         # 1 second\n",
    "                    '500L':0.5,      # 0.5 second\n",
    "                    '100L': 0.1,     # 0.1 seconds\n",
    "                    '10L': 0.01,     # 0.01 seconds\n",
    "                    '1L': 0.001,     # 0.001 seconds\n",
    "                    '100U': 0.001    # 0.001 seconds\n",
    "                }\n",
    "\n",
    "                # Calculate the total seconds to add\n",
    "                total_seconds_to_add = units_to_add * unit_conversion[unit]\n",
    "\n",
    "                # Add the time to the original datetime\n",
    "                return timedelta(seconds=total_seconds_to_add)\n",
    "            # THE BEST AROUND\n",
    "            def save_images_from_dataframes_aligned(asset_data, save_dir, depth, time_interval, window, asset,w):\n",
    "                # Image dimensions\n",
    "                IMAGE_WIDTH = (len(asset_data[asset[0]][0].columns) * 20) + len(asset_data[asset[0]][0].columns)\n",
    "                IMAGE_HEIGHT = int((len(asset_data[asset[0]][0]) / 2) * 4)\n",
    "                # Define pixel boundaries for price values\n",
    "                pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "                def dataframe_to_image_array(df):\n",
    "                    \n",
    "                    # Assuming df is a pandas DataFrame that has been defined earlier\n",
    "                    # Create a copy of the DataFrame to work with, preserving the original\n",
    "                    df_working_copy = df.copy()\n",
    "                    \n",
    "                    IMAGE_WIDTH = (len(df_working_copy.columns) * 20) + len(df_working_copy.columns)\n",
    "                    IMAGE_HEIGHT = int((len(df_working_copy) / 2) * 4) # normalt 4\n",
    "                    pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT)\n",
    "                    \n",
    "                    img = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "                    \n",
    "                    for column in df_working_copy.columns:\n",
    "                        prices = df_working_copy[column].iloc[::2].values\n",
    "                        sizes = np.copy(df_working_copy[column].iloc[1::2].values)  # Work with a copy to avoid modifying the original\n",
    "                    \n",
    "                        price_positions = np.digitize(prices, pixel_boundaries) - 1\n",
    "                    \n",
    "                        # Find all unique values and their counts\n",
    "                        unique_positions, counts = np.unique(price_positions, return_counts=True)\n",
    "                        \n",
    "                        # Identify values that occur more than once\n",
    "                        duplicates = unique_positions[counts > 1]\n",
    "                        \n",
    "                        if duplicates.size > 0:\n",
    "                            for dup in duplicates:\n",
    "                                # Find all indices where this duplicate value occurs\n",
    "                                dup_indices = np.where(price_positions == dup)[0]\n",
    "                                # Sum the sizes for these indices\n",
    "                                size_sum = sizes[dup_indices].sum()\n",
    "                                # Assign the summed size to the duplicates\n",
    "                                sizes[dup_indices] = size_sum\n",
    "                                #print(f\"Column: {column}, Duplicate value {dup} at indices {dup_indices} with adjusted size {size_sum}\")\n",
    "                        #else:\n",
    "                            #print(f\"Column: {column}, No duplicates found\")\n",
    "                        \n",
    "                        # Update the working copy of the DataFrame with the adjusted sizes for this column\n",
    "                        df_working_copy[column].iloc[1::2] = sizes\n",
    "                    \n",
    "                    # rescale the size rows based on the highest value in the column\n",
    "                    \n",
    "                    # Rescaling logic\n",
    "                    \n",
    "                    max_size_value = df_working_copy.loc[df_working_copy.index.str.contains(\"size\")].max().max()\n",
    "                    \n",
    "                    df_working_copy.loc[df_working_copy.index.str.contains(\"size\")] = df_working_copy.loc[df_working_copy.index.str.contains(\"size\")].apply(lambda x: x / max_size_value)\n",
    "                    \n",
    "                    \n",
    "            \n",
    "                    for col_idx, column in enumerate(df_working_copy.columns):\n",
    "                        mid = (col_idx * 21) + 10\n",
    "                        used_positions = set()  # Set to keep track of used price positions\n",
    "                        \n",
    "                        # Extract prices and sizes for the current column\n",
    "                        prices = df_working_copy[column].iloc[::2].values\n",
    "                        sizes = df_working_copy[column].iloc[1::2].values\n",
    "                        price_positions = IMAGE_HEIGHT - np.digitize(prices, pixel_boundaries)\n",
    "                        price_positions = np.clip(price_positions, 0,IMAGE_HEIGHT-1)\n",
    "            \n",
    "                    \n",
    "                        for idx in range(len(prices)):\n",
    "                            price_pos = price_positions[idx]\n",
    "                            #print(f\"Price: {prices[idx]}, Position: {price_pos}\")\n",
    "                            if price_pos not in used_positions:  # Check if the position has not been used\n",
    "                                img[price_pos, mid] = 1  # Mark the price position\n",
    "                                size_value = sizes[idx]\n",
    "                                line_length = int(10 * size_value)  # Determine the length of the size line\n",
    "                                \n",
    "                                # Determine if it's a bid or ask size and position the size line accordingly\n",
    "                                if 'ask_size' in df_working_copy.index[2*idx + 1]:  # This might need adjustment based on your DataFrame's structure\n",
    "                                    img[price_pos, mid+1:mid+1+line_length] = 1  # Position the ask size to the right\n",
    "                                else:\n",
    "                                    img[price_pos, mid-line_length:mid] = 1  # Position the bid size to the left\n",
    "                                \n",
    "                                used_positions.add(price_pos)  # Mark this position as used\n",
    "            \n",
    "            \n",
    "                    return img\n",
    "                # Loop through each asset and each of its dataframes\n",
    "                for asset, dfs in asset_data.items():\n",
    "                    for idx, df_working_copy in enumerate(dfs):\n",
    "                        last_column_name = df_working_copy.columns[-1]  # Extract the name of the first column    \n",
    "                        img_array = dataframe_to_image_array(df_working_copy)\n",
    "                        # Assuming 'last_column_name' is a datetime object\n",
    "                        formatted_timestamp = last_column_name.strftime('%Y-%m-%d_%H-%M-%S.%f%z')\n",
    "                        filename = f\"{asset}_depth{depth}_interval{time_interval}_window{window}_date{w}_{formatted_timestamp}.npz\"\n",
    "                        #filename = f\"{asset}_depth{depth}_interval{time_interval}_window{window}_{last_column_name}.npz\"\n",
    "                        save_path = os.path.join(save_dir, filename)\n",
    "                        np.savez_compressed(save_path, img_array)\n",
    "\n",
    "            asset_results_10ms = scale_asset_dataframes(asset_results_10ms)\n",
    "\n",
    "            save_dir = f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}_date{w}\"\n",
    "            \n",
    "            save_images_from_dataframes_aligned(asset_results_10ms, save_dir, depth, time_interval, window,asset,w)\n",
    "\n",
    "            from datetime import datetime, timedelta\n",
    "            subtime = sub_time_units(time_interval, 400)\n",
    "            addtime = sub_time_units(time_interval, 3500)\n",
    "\n",
    "            #print(asset_results_10ms[asset[0]][-1].columns[-1]- subtime) #timedelta(milliseconds=6000)) # 40ms for 10L; 400ms for 100L\n",
    "            #print(asset_results_10ms[asset[0]][-1].columns[-1]+ addtime) #timedelta(milliseconds=120000)) # 100000ms for 10L;\n",
    "            a = asset_results_10ms[asset[0]][-1].columns[-1] - subtime #timedelta(milliseconds=6000) # 40ms for 10L;\n",
    "            c = asset_results_10ms[asset[0]][-1].columns[-1]\n",
    "            if b < datetime_obj:\n",
    "                b = asset_results_10ms[asset[0]][-1].columns[-1] + addtime #timedelta(milliseconds=120000) # 100000ms for 10L;\n",
    "            else:\n",
    "                c = datetime_obj  \n",
    "\n",
    "        else :\n",
    "            df_mid = concatenate_orderbooks(asset, date,level,directory)\n",
    "            # Assuming df_mid is your DataFrame\n",
    "\n",
    "            # Convert the 'ts' column to datetime if it's not already\n",
    "            df_mid['ts'] = pd.to_datetime(df_mid['ts'])\n",
    "\n",
    "            # Set the 'ts' column as the index\n",
    "            df_mid.set_index('ts', inplace=True)\n",
    "\n",
    "            ask_price_1_df = df_mid['ask_price_1'].resample(time_interval).last()\n",
    "            ask_size_1_df = df_mid['ask_size_1'].resample(time_interval).last()\n",
    "            bid_price_1_df = df_mid['bid_price_1'].resample(time_interval).last()\n",
    "            bid_size_1_df = df_mid['bid_size_1'].resample(time_interval).last()\n",
    "            midquote_df = df_mid['midquote'].resample(time_interval).last()\n",
    "\n",
    "            # Resample the DataFrame to a 10-millisecond interval\n",
    "            # and use the last midquote observation for each interval\n",
    "            resampled_df = df_mid['midquote'].resample(time_interval).last()\n",
    "\n",
    "            # Calculate returns as the percentage change in midquote\n",
    "            returns = resampled_df.pct_change(periods=prediction_ahead)\n",
    "\n",
    "            # Forward fill missing data\n",
    "            returns_filled = returns.fillna(method='ffill')\n",
    "\n",
    "            # Convert the returns back to a DataFrame with the timestamp reset as a column\n",
    "            returns_df = returns_filled.reset_index()\n",
    "\n",
    "            # wish to move the midqoute column one down in the returns_df\n",
    "            # make the the prediction!!!\n",
    "            returns_df['midquote'] = returns_df['midquote'].shift(-prediction_ahead) #burde vre 4 her!!! (jvnfr tanken i frste omgang)\n",
    "\n",
    "            # do it for the ask_price_1_df:\n",
    "            ask_price_1_df = ask_price_1_df.reset_index().rename(columns={'ts': 'ask_price_1'})\n",
    "            ask_price_1_df.columns = ['ts', 'ask_price_1']\n",
    "            ask_price_1_df = ask_price_1_df.fillna(method='ffill')\n",
    "\n",
    "            # do it for the ask_size_1_df:\n",
    "            ask_size_1_df = ask_size_1_df.reset_index().rename(columns={'ts': 'ask_size_1'})\n",
    "            ask_size_1_df.columns = ['ts', 'ask_size_1']\n",
    "            ask_size_1_df = ask_size_1_df.fillna(method='ffill')\n",
    "\n",
    "            # do it for the bid_price_1_df:\n",
    "            bid_price_1_df = bid_price_1_df.reset_index().rename(columns={'ts': 'bid_price_1'})\n",
    "            bid_price_1_df.columns = ['ts', 'bid_price_1']\n",
    "            bid_price_1_df = bid_price_1_df.fillna(method='ffill')\n",
    "\n",
    "            # do it for the bid_size_1_df:\n",
    "            bid_size_1_df = bid_size_1_df.reset_index().rename(columns={'ts': 'bid_size_1'})\n",
    "            bid_size_1_df.columns = ['ts', 'bid_size_1']\n",
    "            bid_size_1_df = bid_size_1_df.fillna(method='ffill')\n",
    "\n",
    "            # do it for the midquote_df:\n",
    "            midquote_df = midquote_df.reset_index().rename(columns={'ts': 'midquote'})\n",
    "            midquote_df.columns = ['ts', 'midquote_real']\n",
    "            midquote_df = midquote_df.fillna(method='ffill')\n",
    "            import pandas as pd\n",
    "            from functools import reduce\n",
    "\n",
    "            # Assuming you have six DataFrames: df1, df2, df3, df4, df5, df6\n",
    "            dataframes = [ask_price_1_df, ask_size_1_df, bid_price_1_df, bid_size_1_df, midquote_df, returns_df]\n",
    "\n",
    "            # Merge all DataFrames on a common column using reduce\n",
    "            returns_df = reduce(lambda left, right: pd.merge(left, right, on='ts', how='inner'), dataframes)\n",
    "\n",
    "            # Now merged_df contains data from all six DataFrames, merged on 'common_column'\n",
    "            # Move the ts column one up in the returns_df\n",
    "            returns_df['ts'] = returns_df['ts'].shift(-1)\n",
    "\n",
    "            ## Create a DataFrame by combining the six Series\n",
    "            #df = pd.DataFrame({\n",
    "            #    'Column1': returns_df, \n",
    "            #    'Column2': ask_price_1_df, \n",
    "            #    'Column3': ask_size_1_df, \n",
    "            #    'Column4': bid_price_1_df, \n",
    "            #    'Column5': bid_size_1_df, \n",
    "            #    'Column6': midquote_df\n",
    "            #})\n",
    "\n",
    "            #display(df)\n",
    "\n",
    "            # make a new new_datetime_start that add 5 units to the original_starttime\n",
    "            new_datetime_start = add_time_units(original_starttime, time_interval, window)  # Adds window sequence to the new_datetime_start\n",
    "            # make the the dataframe start from the first_formatted_timestamp\n",
    "            new_datime_end = minus_time_units(datetime_obj, time_interval, window) # Minus window sequence to the new_datetime_start\n",
    "\n",
    "            # make the the dataframe start from the new_datetime_start to the new_datime_end\n",
    "            returns_df = returns_df.loc[returns_df['ts'].between(new_datetime_start, new_datime_end)].reset_index(drop=True)\n",
    "\n",
    "            # pu the the returns_df to the df_returns by concat\n",
    "            #df_returns = pd.concat([df_returns, returns_df])\n",
    "\n",
    "            #reindex the df_returns\n",
    "            returns_df = returns_df.reset_index(drop=True)\n",
    "            # save the returns_df to csv\n",
    "            returns_df.to_csv(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/Returns/depth{depth}_time{time_interval}_window{window}_date{w}/returns_{i}_.csv\", index=False)\n",
    "            print(f\"Last value of b is: {b}\")\n",
    "            print(\"Done with the asset:\", i)\n",
    "    print('Done with the date:', w) \n",
    "print(\"Done with all the dates\")\n",
    "\n",
    "# remove the ones that are not needed\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Define the directory where the files are stored\n",
    "directory = f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}_date{w}\"\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Iterate through the files and delete those with timestamps greater than datetime_obj\n",
    "for file_name in files:\n",
    "    #print the last 20 characters of the file name\n",
    "    name_of_file = file_name[-35:-4]\n",
    "    #convert name_of_file to time format:\n",
    "    # Define the format for parsing\n",
    "    format_str = '%Y-%m-%d_%H-%M-%S.%f%z'\n",
    "    # Convert the string to a datetime object\n",
    "    to_time = datetime.strptime(name_of_file, format_str)\n",
    "    \n",
    "    if to_time > datetime_obj:\n",
    "        # Define the path of the file to delete\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        # Delete the file\n",
    "        os.remove(file_path)\n",
    "print(\"All files removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the code above danymic - done\n",
    "# Get the data from calling a function (get the returns and the images)\n",
    "# Vr obs p at dybden p nogle stocks ikke er s stor -> s den str som 9999999 (Dette skal der tages hnd om) -> mske bare lav en function til at fange maks dybden for tidshorisonten\n",
    "# Vr obs p tispunktet b ikke overstiger 24.00.00 -> s det bliver den nste dag (Dette skal der tages hnd om)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNT THE NUMBER OF FILES IN A FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import os\n",
    "#ORIGINAL\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TM', 'TMO', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UPS', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'ZTS']#['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS'] \n",
    "\n",
    "level = 15\n",
    "prediction_ahead = 4 # This the the duration of each picture 4*time_interval \n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '10L' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 500L=0,5sek, 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10\n",
    "date_listen = [\"2023-03-21\"]#[\"2023-03-21\",\"2023-03-22\",\"2023-03-23\"]\n",
    "\n",
    "def count_files_fast(folder_path):\n",
    "    \"\"\"Counts the number of files in the specified folder using os.scandir(), which is faster for large directories.\"\"\"\n",
    "    count = 0\n",
    "    with os.scandir(folder_path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# Initialize the sum of file counts\n",
    "total_file_count = 0\n",
    "# Loop through each date\n",
    "for w in date_listen:\n",
    "    folder_path = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}_date{w}'\n",
    "    file_count = count_files_fast(folder_path)\n",
    "    total_file_count += file_count\n",
    "    print(f'Number of files in folder for {w}: {file_count}')\n",
    "\n",
    "# Print the total number of files across all folders\n",
    "print(f'Total number of files across all folders: {total_file_count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE THE DF_ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_df_array(depth, time_interval, window,date_listen):\n",
    "    \"\"\"\n",
    "    Load .npz files from a specified folder and process the data into a pandas DataFrame.\n",
    "\n",
    "    This function reads all .npz files from a given directory, concatenates arrays found within\n",
    "    each file, and then extracts relevant information like 'Ticker' and 'Time' from the filenames.\n",
    "    The function assumes a specific naming convention for the files.\n",
    "\n",
    "    Parameters:\n",
    "    depth (int): Depth parameter used in the folder path.\n",
    "    time_interval (int): Time interval parameter used in the folder path.\n",
    "    window (int): Window parameter used in the folder path.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the filenames, combined arrays, tickers, and times.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    # Define the folder path using formatted strings for depth, time_interval, and window\n",
    "    for w in date_listen:\n",
    "        folder_path = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}_date{w}'\n",
    "\n",
    "        # List all .npz files in the specified folder\n",
    "        file_list = [file for file in os.listdir(folder_path) if file.endswith('.npz')]\n",
    "        file_list.sort()  # Sort the file list alphabetically\n",
    "\n",
    "        # Lists to store each file's numpy array and filename\n",
    "        arrays = []\n",
    "        filenames = []\n",
    "\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with np.load(file_path, allow_pickle=True) as data:\n",
    "                # Combine all arrays in a file, if any\n",
    "                combined_array = np.concatenate([data[array_name] for array_name in data.files])\n",
    "                arrays.append(combined_array)\n",
    "                filenames.append(file)\n",
    "\n",
    "        # Create a DataFrame with filenames and arrays\n",
    "        df_array = pd.DataFrame({'Filename': filenames, 'Arrays': arrays})\n",
    "\n",
    "        # Extract ticker and time from the filename, and convert time to datetime format\n",
    "        df_array['Ticker'] = df_array['Filename'].str.extract(r'([A-Z]+)_')\n",
    "        df_array['Time'] = pd.to_datetime(df_array['Filename'].str.extract(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}\\.\\d{6})')[0], format='%Y-%m-%d_%H-%M-%S.%f')\n",
    "\n",
    "        def minus_time_units(original_starttime, unit, units_to_add):\n",
    "            # Define the conversion of each unit to seconds\n",
    "            unit_conversion = {\n",
    "                '60S': 60,         # 60 seconds\n",
    "                '30S': 30,         # 30 seconds\n",
    "                '10S': 10,         # 10 second\n",
    "                '5S': 5,         # 5 second\n",
    "                '1S': 1,         # 1 second\n",
    "                '500L':0.5,      # 0.5 second\n",
    "                '100L': 0.1,     # 0.1 seconds\n",
    "                '10L': 0.01,     # 0.01 seconds\n",
    "                '1L': 0.001,     # 0.001 seconds\n",
    "                '100U': 0.0001    # 0.001 seconds\n",
    "            }\n",
    "            # Calculate the total seconds to add\n",
    "            total_seconds_to_add = units_to_add * unit_conversion[unit]\n",
    "            # Add the time to the original datetime\n",
    "            return original_starttime - timedelta(seconds=total_seconds_to_add)\n",
    "\n",
    "        datetime_obj = datetime.fromisoformat(f\"{w} 09:33:00.000+00:00\") # default is 16:00:00.000+00:00\n",
    "        # Drop some rows that are not needed \n",
    "        new_datime_end = minus_time_units(datetime_obj, time_interval, window)\n",
    "        new_datime_end = new_datime_end.replace(tzinfo=None)\n",
    "        df_array = df_array[df_array['Time'] <= new_datime_end]\n",
    "        \n",
    "        # count the rows in the df_array\n",
    "        print(len(df_array))\n",
    "\n",
    "        all_dfs.append(df_array)\n",
    "\n",
    "    # Concatenate all DataFrames from the list\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "df_array = generate_df_array(depth, time_interval, window,date_listen)\n",
    "df_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(df_array['Arrays'][23], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE THE DF_MIDPRICE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def generate_df_midprice(depth, time_interval, window,date_listen):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame (df_midprice) from CSV files in a specified folder.\n",
    "    This function reads CSV files from a given directory, extracts ticker information,\n",
    "    processes time data, renames columns as necessary, and creates a 'midquote_target' column.\n",
    "\n",
    "    Parameters:\n",
    "    depth (int): Depth parameter used in the folder path.\n",
    "    time_interval (int): Time interval parameter used in the folder path.\n",
    "    window (int): Window parameter used in the folder path.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing data from all CSV files with additional columns for Ticker, Time, and midquote_target.\n",
    "    \"\"\"\n",
    "    dataframe_collect = []\n",
    "    \n",
    "    for w in date_listen:\n",
    "        # Define the folder path using formatted strings for depth, time_interval, and window\n",
    "        folder_path = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/Returns/depth{depth}_time{time_interval}_window{window}_date{w}'\n",
    "\n",
    "        # List and sort CSV files in the specified folder\n",
    "        csv_files = sorted([file for file in os.listdir(folder_path) if file.endswith('.csv')])\n",
    "\n",
    "        dataframes = []  # Initialize an empty list to store DataFrames\n",
    "\n",
    "        for file in csv_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract ticker from the file name\n",
    "            Ticker = ''.join(filter(str.isupper, os.path.splitext(file)[0]))\n",
    "            df['Ticker'] = Ticker  # Add the ticker as a new column\n",
    "\n",
    "            dataframes.append(df)\n",
    "\n",
    "        # Concatenate all DataFrames into one\n",
    "        df_midprice = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "        # Reorder columns to have 'Ticker' first\n",
    "        df_midprice = df_midprice[['Ticker'] + [col for col in df_midprice.columns if col != 'Ticker']]\n",
    "\n",
    "        # Rename the 'ts' column to 'Time' if it exists\n",
    "        if 'ts' in df_midprice.columns:\n",
    "            df_midprice = df_midprice.rename(columns={'ts': 'Time'})\n",
    "\n",
    "        def standardize_timestamps(timestamp_str):\n",
    "            try:\n",
    "                # Parse the timestamp string to a datetime object\n",
    "                timestamp_obj = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "                # Reformat to include microseconds (even if zero)\n",
    "                return timestamp_obj.strftime('%Y-%m-%d %H:%M:%S.%f%z')\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    # Attempt parsing without timezone if the first format fails\n",
    "                    timestamp_obj = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')\n",
    "                    return timestamp_obj.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "                except ValueError:\n",
    "                    # Return the original string if both parsing attempts fail\n",
    "                    return timestamp_str\n",
    "\n",
    "        # Apply the function to each element in the 'Time' column\n",
    "        df_midprice['Time'] = df_midprice['Time'].apply(standardize_timestamps)\n",
    "\n",
    "\n",
    "        # Convert 'Time' column to datetime64[ns], removing timezone\n",
    "        df_midprice['Time'] = pd.to_datetime(df_midprice['Time']).dt.tz_localize(None)\n",
    "\n",
    "        # Convert 'Ticker' column to string\n",
    "        df_midprice['Ticker'] = df_midprice['Ticker'].astype(str)\n",
    "\n",
    "        # Create a new column 'midquote_target' based on the 'midquote' values\n",
    "        df_midprice['midquote_target'] = np.where(df_midprice['midquote'] > 0, 1, 0)\n",
    "        print(len(df_midprice))\n",
    "        dataframe_collect.append(df_midprice)\n",
    "    \n",
    "    df_midprice = pd.concat(dataframe_collect, ignore_index=True) if dataframe_collect else pd.DataFrame()\n",
    "    return df_midprice\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df_midprice = generate_df_midprice(depth, time_interval, window, date_listen)\n",
    "df_midprice\n",
    "# remove duplicates rows based on the 'Time' and 'Ticker' columns\n",
    "#df_midprice = df_midprice.drop_duplicates(subset=['Time', 'Ticker'], keep='first')\n",
    "#df_midprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINE THE DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS FOR THE ONCE THAT DO NOT HAVE THE SAME START TIME\n",
    "\n",
    "## Find the rows in df_array that are not in df_midprice\n",
    "df_array_not_in_df_midprice = df_array[~df_array.set_index(['Time', 'Ticker']).index.isin(df_midprice.set_index(['Time', 'Ticker']).index)]\n",
    "#\n",
    "## Find the rows in df_midprice that are not in df_array\n",
    "df_midprice_not_in_df_array = df_midprice[~df_midprice.set_index(['Time', 'Ticker']).index.isin(df_array.set_index(['Time', 'Ticker']).index)]\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all the following tickers from the df_array: 'AJG', 'AMGN', 'AON', 'APD', 'AZO', 'CB', 'CHTR', 'CI', 'CME','CNI', 'CTAS', 'DELL', 'DHR', 'EQIX', 'FMX', 'INTU', 'MMC', 'MSI','NOC', 'PH', 'RSG', 'SCCO', 'SMFG', 'SONY', 'SPGI', 'TMO', 'TM','TT', 'UPS', 'WDAY','AMX'\n",
    "df_array = df_array[~df_array['Ticker'].isin(['AJG', 'AMGN', 'AON', 'APD', 'AZO', 'CB', 'CHTR', 'CI', 'CME','CNI', 'CTAS', 'DELL', 'DHR', 'EQIX', 'FMX', 'INTU', 'MMC', 'MSI','NOC', 'PH', 'RSG', 'SCCO', 'SMFG', 'SONY', 'SPGI', 'TMO', 'TM','TT', 'UPS', 'WDAY','AMX'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(df_array,df_midprice):\n",
    "    # combine the df_array and df_midprice\n",
    "    if len(df_midprice) == len(df_array):\n",
    "        df = pd.merge(df_midprice, df_array, on=['Ticker', 'Time'])\n",
    "    else:\n",
    "        print(\"ERROR: The length of df_midprice is not equal to the length of df_array\")\n",
    "    return df\n",
    "df_combined = combine_dataframes(df_array,df_midprice)\n",
    "# make the spread column:\n",
    "df_combined['spread'] = df_combined['ask_price_1'] - df_combined['bid_price_1']\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df_combined to to pickle\n",
    "df_combined.to_pickle(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/combined/combined_depth{depth}_time{time_interval}_window{window}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE WITH THE RIGHT PICTURES\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# BMO+XOM \n",
    "\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']  #['AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AEP', 'ALGN', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'ANSS', 'ASML', 'AVGO', 'AZN', 'BIIB', 'BKNG', 'BKR', 'CDNS', 'CEG', 'CHTR', 'CMCSA', 'COST', 'CPRT', 'CRWD', 'CSCO', 'CSGP', 'CSX', 'CTAS', 'CTSH', 'DDOG', 'DLTR', 'DXCM', 'EA', 'EBAY', 'ENPH', 'EXC', 'FANG', 'FAST', 'FTNT', 'GEHC', 'GFS', 'GILD', 'GOOGL', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'JD', 'KDP', 'KHC', 'KLAC', 'LCID', 'LRCX', 'LULU', 'MAR', 'MDLZ', 'MELI', 'META', 'MNST', 'MRNA', 'MRVL', 'MSFT', 'MU', 'NFLX', 'NVDA', 'NXPI', 'ODFL', 'ON', 'ORLY', 'PANW', 'PAYX', 'PCAR', 'PDD', 'PEP', 'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SGEN', 'SIRI', 'SNPS', 'TEAM', 'TMUS', 'TSLA', 'TXN', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZM', 'ZS'] # ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'APO', 'ARM', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BRK.B', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CB', 'CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'E', 'ECL', 'EL', 'ELV', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FI', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITUB', 'ITW', 'JNJ', 'JPM', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'MUFG', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PBR', 'PBR.A', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RTX', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPGI', 'STLA', 'SYK', 'T', 'TD', 'TDG', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "\n",
    "#original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")\n",
    "\n",
    "level = 15\n",
    "prediction_ahead = 4 # This the the duration of each picture 4*time_interval \n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '10L' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 500L=0,5sek, 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10\n",
    "date_listen = [\"2023-03-21\"]#[\"2023-03-21\",\"2023-03-22\",\"2023-03-23\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/combined/combined_depth{depth}_time{time_interval}_window{window}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_of_interest = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'AFL','AIG', 'ALL', 'AMAT', 'AMD', 'AMT', 'AMZN', 'ANET', 'ASML', 'AVGO','AXP', 'AZN', 'BABA', 'BAC', 'BA', 'BBVA', 'BDX', 'BHP', 'BKNG','BLK', 'BMY', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'CARR','CCI', 'CDNS', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CP', 'CRH','CRM', 'CSCO', 'CSX', 'CVS', 'CVX', 'C', 'DASH', 'DEO','DE', 'DHI', 'DIS', 'DUK', 'EL', 'EMR', 'ENB', 'EOG', 'EPD','EQNR', 'ETN', 'ET', 'EW', 'FCX', 'FTNT', 'GD', 'GE', 'GILD', 'GM','GOOGL', 'GSK', 'GWW', 'HCA', 'HDB', 'HD', 'HLT', 'HMC', 'HON','HSBC', 'HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'ISRG','ITW', 'JNJ', 'JPM', 'KHC', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY','LMT', 'LOW', 'LRCX', 'LULU', 'MAR', 'MA', 'MCD', 'MCHP', 'MCO','MDLZ', 'MDT', 'MELI', 'META', 'MET', 'MMM', 'MNST', 'MO', 'MPC','MRK', 'MRVL', 'MSCI', 'MSFT', 'MS', 'MU', 'NEE', 'NFLX', 'NGG','NKE', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI','ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX', 'PBR', 'PCAR', 'PEP','PFE', 'PGR', 'PG', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD','PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROST', 'RY', 'SAN','SAP', 'SBUX', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SNOW','SNPS', 'SNY', 'SO', 'SPG', 'SPOT', 'STLA', 'SYK', 'TD', 'TEAM','TFC', 'TGT', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TTE', 'TXN', 'T','UBER', 'UBS', 'UL', 'UNH', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ','WELL', 'WFC', 'WM', 'ZTS']\n",
    "\n",
    "len(tickers_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from the pickle file\n",
    "df = pd.read_pickle(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/combined/combined_depth{depth}_time{time_interval}_window{window}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display this picture df['Arrays'][76]\n",
    "import matplotlib.pyplot as plt\n",
    "# make the plot bigger\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.imshow(df['Arrays'][4393], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_dataframe_by_ticker(df, time_column='Time', train_ratio=0.6, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training, validation, and test sets based on ticker.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    time_column (str): The name of the column containing time data.\n",
    "    train_ratio (float): The proportion of data to be used for training.\n",
    "    val_ratio (float): The proportion of data to be used for validation.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Training set.\n",
    "    DataFrame: Validation set.\n",
    "    DataFrame: Test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Time column to datetime for proper sorting\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "    # Define a function to split the data\n",
    "    def split_data(group):\n",
    "        group = group.sort_values(by=time_column)\n",
    "        idx_train = int(len(group) * train_ratio)\n",
    "        idx_val = int(len(group) * (train_ratio + val_ratio))\n",
    "        return group.iloc[:idx_train], group.iloc[idx_train:idx_val], group.iloc[idx_val:]\n",
    "\n",
    "    # Apply the function and get splits\n",
    "    splits = df.groupby('Ticker').apply(lambda g: split_data(g))\n",
    "\n",
    "    # Extract splits into separate DataFrames using list comprehension\n",
    "    train_df = pd.concat([s[0] for s in splits])\n",
    "    val_df = pd.concat([s[1] for s in splits])\n",
    "    test_df = pd.concat([s[2] for s in splits])\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Usage Example:\n",
    "train_df, val_df, test_df = split_dataframe_by_ticker(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation of Sharpe Ratio (maybe insert this into the test file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import numpy as np\n",
    "import os\n",
    "#ORIGINAL\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "df_returns = pd.DataFrame()\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TM', 'TMO', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UPS', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'ZTS']\n",
    "\n",
    "level = 15\n",
    "prediction_ahead = 4 # This the the duration of each picture 4*time_interval \n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '10L' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 500L=0,5sek, 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10\n",
    "date_listen = [\"2023-03-21\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/combined/combined_depth{depth}_time{time_interval}_window{window}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2189600\n",
      "1459528\n",
      "1121000\n"
     ]
    }
   ],
   "source": [
    "# only keep the rows there the ticker column is in the asset_list:\n",
    "df = df[df['Ticker'].isin(asset_list)]\n",
    "\n",
    "#sort df first by 'Time' and then by 'Ticker':\n",
    "df = df.sort_values(by=['Time', 'Ticker'])\n",
    "\n",
    "training_procent = 0.6\n",
    "\n",
    "# Calculate the number of samples that should be in the training set\n",
    "num_samples = len(df) * training_procent\n",
    "\n",
    "def closest_value(input):\n",
    "    # Adjust the input to the closest higher multiple of 250\n",
    "    return int(input + (200 - input % 200))\n",
    "\n",
    "# Get the adjusted number of samples for the training set\n",
    "train_samples = closest_value(num_samples)\n",
    "\n",
    "# Split the dataframe\n",
    "train_df = df[:train_samples]\n",
    "test_df = df[train_samples:]\n",
    "\n",
    "print(len(train_df))  # This will print the number of samples in the training set\n",
    "print(len(test_df))   # This will print the number of samples in the test set\n",
    "\n",
    "test_df = test_df.iloc[:1121000]\n",
    "\n",
    "print(len(test_df))   # This will print the number of samples in the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NY TEST AF MODEL KR DETTE I COLAB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 40 * 105, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch_size, 40, 105)\n",
    "        x = x.unsqueeze(1)  # Add a channel dimension (batch_size, 1, 40, 105)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "# Define the loss function and optimizer\n",
    "# Define the model (using the SimpleCNN class from the previous example)\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Add code here to prepare your data, train, and evaluate the model\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.tensors = list(dataframe['tensors'])\n",
    "        self.targets = list(dataframe['midquote_target'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensors[idx], self.targets[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(train_df)\n",
    "val_dataset = TensorDataset(val_df)\n",
    "test_dataset = TensorDataset(test_df)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SimpleCNN().to(device)  # Replace SimpleCNN with your model class\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10  # Define the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.float())\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that there are the same amount of positive and negative returns in the training data, randomly choose the same amount of positive and negative returns\n",
    "# make a dataframe with only the positive returns\n",
    "df_positive = train_df[train_df['midquote_target'] == 1]\n",
    "# make a dataframe with only the negative returns\n",
    "df_negative = train_df[train_df['midquote_target'] == 0]\n",
    "\n",
    "# find the length of the positive dataframe\n",
    "length_positive = len(df_positive)\n",
    "# find the length of the negative dataframe\n",
    "length_negative = len(df_negative)\n",
    "\n",
    "# find the minimum length of the two dataframes\n",
    "min_length = min(length_positive, length_negative)\n",
    "\n",
    "# randomly choose the same amount of positive and negative returns\n",
    "df_positive_fixed = df_positive.sample(n=min_length, random_state=1)\n",
    "df_negative_fixed = df_negative.sample(n=min_length, random_state=1) \n",
    "\n",
    "# combine the two dataframes\n",
    "\n",
    "df_train = pd.concat([df_positive_fixed, df_negative_fixed])\n",
    "\n",
    "print('lenght of the df_train:', len(df_train))\n",
    "print('lenght of the df_val:', len(val_df))\n",
    "print('lenght of the df_test:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me distribution of the midquote that are above 0 and the ones that are below 0 or equal to 0:\n",
    "print(df_train[df_train['midquote'] > 0]['midquote'].count()/len(df_train['midquote']))\n",
    "print(df_train[df_train['midquote'] <= 0]['midquote'].count()/len(df_train['midquote']))\n",
    "print(f\"Number of neutral midquotes: {df_train[df_train['midquote'] == 0]['midquote'].count()} out of {len(df_train['midquote'])}\")\n",
    "\n",
    "# make a distribution plot of the midquote\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.displot(df_train['midquote'], bins=100, kde=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUSH TO COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "# Define destination folder path\n",
    "destination_folder = '/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/tar_folder/'\n",
    "\n",
    "# Usage for first tar file\n",
    "source_folder1 = f'/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}'\n",
    "output_tar_file1 = destination_folder + 'array.tar'  # Save to destination folder\n",
    "\n",
    "make_tarfile(output_tar_file1, source_folder1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "SERVICE_ACCOUNT_FILE = 'credentials.json'\n",
    "PARENT_FOLDER_ID = \"1ARfYF9vuYgHeoffm6I0ezfKU11eAyByI\"\n",
    "\n",
    "def authenticate():\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def upload_file(file_path):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_name = os.path.basename(file_path)  # Extracts file name from file_path\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [PARENT_FOLDER_ID]\n",
    "    }\n",
    "\n",
    "    media = MediaFileUpload(file_path, resumable=True)\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "\n",
    "def upload_folder(folder_path):\n",
    "    for item in os.listdir(folder_path):\n",
    "        full_item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(full_item_path):\n",
    "            upload_file(full_item_path)\n",
    "\n",
    "# Usage example: upload all files in a specific local folder\n",
    "local_folder_path = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/tar_folder\"  # Replace with your local folder path\n",
    "upload_folder(local_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "import os\n",
    "\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "SERVICE_ACCOUNT_FILE = 'credentials.json'\n",
    "PARENT_FOLDER_ID = \"1J633912fL3GnUOyWpWgaoTmdm0DM3RhS\"\n",
    "\n",
    "def authenticate():\n",
    "    creds = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    return creds\n",
    "\n",
    "def upload_file(file_path):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_name = os.path.basename(file_path)  # Extracts file name from file_path\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [PARENT_FOLDER_ID]\n",
    "    }\n",
    "\n",
    "    media = MediaFileUpload(file_path, resumable=True)\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "\n",
    "def upload_folder(folder_path):\n",
    "    for item in os.listdir(folder_path):\n",
    "        full_item_path = os.path.join(folder_path, item)\n",
    "        if os.path.isfile(full_item_path):\n",
    "            upload_file(full_item_path)\n",
    "# Usage example: upload all files in a specific local folder\n",
    "local_folder_path = f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/Returns/depth{depth}_time{time_interval}_window{window}\"  # Replace with your local folder path\n",
    "upload_folder(local_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "df_returns = pd.DataFrame()\n",
    "asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'ALL','AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CARR','CB', 'CCI','CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DASH','DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'ECL', 'EL', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK','GWW', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC','HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITW', 'JNJ', 'JPM', 'KHC','KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCHP', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX','PBR', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPG','SPGI', 'SPOT','STLA', 'SYK', 'T', 'TD', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS']\n",
    "original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")\n",
    "\n",
    "level = 15\n",
    "prediction_ahead = 4\n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '5S' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10\n",
    "\n",
    "# import the pickle file\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/combined/combined_depth{depth}_time{time_interval}_window{window}.pkl\")\n",
    "\n",
    "#sort df first by 'Time' and then by 'Ticker':\n",
    "df = df.sort_values(by=['Time', 'Ticker'])\n",
    "\n",
    "\n",
    "x = len(df)*0.6 \n",
    "\n",
    "def closest_value(x):\n",
    "    return int(x + 250 - x%250)\n",
    "\n",
    "input = closest_value(x)\n",
    "\n",
    "# keep 60 pct of the data as training data and 40 pct as test data\n",
    "train_df = df[:int(input)]\n",
    "test_df = df[int(1-input):]\n",
    "\n",
    "use_gpu = True\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "if use_gpu:\n",
    "    from gpu_tools import *\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize an empty list to store batches\n",
    "batched_images = []\n",
    "\n",
    "# Process arrays in batches\n",
    "for i in range(0, len(train_df), batch_size):\n",
    "    batch = train_df['Arrays'].iloc[i:i+batch_size]\n",
    "    batched_images.append(np.stack(batch))\n",
    "\n",
    "# Concatenate batches to create the final array\n",
    "images = np.concatenate(batched_images)\n",
    "\n",
    "label_df = train_df\n",
    "\n",
    "# Check the shape of the resulting array\n",
    "print(images.shape)\n",
    "print(label_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_ramdon_split:\n",
    "    train_val_ratio = 0.7\n",
    "    split_idx = int(images.shape[0] * 0.7)\n",
    "    train_dataset = MyDataset(images[:split_idx], (label_df.midquote_target).values[:split_idx])\n",
    "    val_dataset = MyDataset(images[split_idx:], (label_df.midquote_target).values[split_idx:])\n",
    "else:\n",
    "    dataset = MyDataset(images, (label_df.midquote_target).values)\n",
    "    train_val_ratio = 0.7\n",
    "    train_dataset, val_dataset = random_split(dataset, \\\n",
    "        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n",
    "        generator=torch.Generator().manual_seed(42))\n",
    "    del dataset\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1), stride=(2, 1)),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1), stride=(2, 1)),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n",
    "            nn.MaxPool2d((2, 1), stride=(2, 1)),\n",
    "        )\n",
    "        \n",
    "        # Dynamically calculate the size of the FC layer\n",
    "        with torch.no_grad():\n",
    "            # Correctly prepare the dummy input tensor\n",
    "            dummy_input = torch.autograd.Variable(torch.rand(1, 1, 40, 105))\n",
    "            self._temp_size = self._get_conv_output(dummy_input).view(-1).shape[0]\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self._temp_size, 2),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output(self, input_tensor):\n",
    "        output = self._forward_features(input_tensor)\n",
    "        return output\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "export_onnx = True\n",
    "net = Net().to(device)\n",
    "net.apply(init_weights)  # Ensure init_weights is defined elsewhere\n",
    "\n",
    "if export_onnx:\n",
    "    import torch.onnx\n",
    "    # Adjust the dummy input to match your actual input size and device\n",
    "    x = torch.randn([1, 1, 40, 105]).to(device)\n",
    "    torch.onnx.export(net, x, \"../cnn_baseline.onnx\", export_params=True,  # Changed to export_params=True to include weights\n",
    "                      opset_version=10, do_constant_folding=True,  # Adjusted for optimization\n",
    "                      input_names=['input_images'], output_names=['output_prob'],\n",
    "                      dynamic_axes={'input_images': {0: 'batch_size'}, 'output_prob': {0: 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataloader:\n",
    "    # Add a channel dimension to make it [batch_size, 1, height, width]\n",
    "    images = images.unsqueeze(1)  # This changes shape from [128, 40, 105] to [128, 1, 40, 105]\n",
    "    # Now you can forward pass this through your network\n",
    "    outputs = net(images.to(device))\n",
    "    # Rest of your training loop...\n",
    "    break  # This break is just to stop the loop here for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile as thop_profile\n",
    "# Creating a dummy input tensor with the correct shape [1, 1, 40, 105]\n",
    "# This assumes a single sample, but you can adjust the first dimension as needed\n",
    "input_tensor = torch.randn(1, 40, 105).unsqueeze(1).to(device)  # Now shape is [1, 1, 40, 105]\n",
    "\n",
    "# Profiling with THOP\n",
    "flops, params = thop_profile(net, inputs=(input_tensor,))\n",
    "print('FLOPs = ' + str(flops / 1000**3) + 'G')\n",
    "print('Params = ' + str(params / 1000**2) + 'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, net, loss_fn, optimizer):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.train()\n",
    "    \n",
    "    with tqdm(dataloader) as t:\n",
    "        for batch, (X, y) in enumerate(t):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = net(X)\n",
    "            loss = loss_fn(y_pred, y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n",
    "            current += len(X)\n",
    "            t.set_postfix({'running_loss':running_loss})\n",
    "    \n",
    "    return running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(dataloader, net, loss_fn):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    current = 0\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader) as t:\n",
    "            for batch, (X, y) in enumerate(t):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = net(X)\n",
    "                loss = loss_fn(y_pred, y.long())\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n",
    "                current += len(X)\n",
    "            \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.unsqueeze(1)  # Ensure X has the correct shape\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Convert target tensor to long\n",
    "        y = y.long()  # This line fixes the error\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Avg training loss: {average_loss}\")\n",
    "    return average_loss\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.unsqueeze(1)  # Ensure X has the correct shape\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.long()  # Convert target tensor to long\n",
    "\n",
    "            pred = model(X)\n",
    "            total_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    print(f\"Avg validation loss: {average_loss}\")\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n",
    "    net = net.to(device)\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "\n",
    "start_epoch = 0\n",
    "min_val_loss = 1e9\n",
    "last_min_ind = -1\n",
    "early_stopping_epoch = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now().strftime('%Y%m%d_%H:%M:%S')\n",
    "#os.mkdir('../pt'+os.sep+start_time)\n",
    "epochs = 100\n",
    "for t in range(start_epoch, epochs):\n",
    "    print(f\"Epoch {t}\\n-------------------------------\")\n",
    "    time.sleep(0.2)\n",
    "    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = val_loop(val_dataloader, net, loss_fn)\n",
    "    tb.add_histogram(\"train_loss\", train_loss, t)\n",
    "    torch.save(net, '../pt'+os.sep+start_time+os.sep+'baseline_epoch_{}_train_{:5f}_val_{:5f}.pt'.format(t, train_loss, val_loss)) \n",
    "    if val_loss < min_val_loss:\n",
    "        last_min_ind = t\n",
    "        min_val_loss = val_loss\n",
    "    elif t - last_min_ind >= early_stopping_epoch:\n",
    "        break\n",
    "\n",
    "print('Done!')\n",
    "print('Best epoch: {}, val_loss: {}'.format(last_min_ind, min_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "if use_gpu:\n",
    "    from utils.gpu_tools import *\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize an empty list to store batches\n",
    "batched_images = []\n",
    "\n",
    "# Process arrays in batches\n",
    "for i in range(0, len(test_df), batch_size):\n",
    "    batch = test_df['Arrays'].iloc[i:i+batch_size]\n",
    "    batched_images.append(np.stack(batch))\n",
    "\n",
    "# Concatenate batches to create the final array\n",
    "images = np.concatenate(batched_images)\n",
    "\n",
    "label_df = test_df\n",
    "\n",
    "# Check the shape of the resulting array\n",
    "print(images.shape)\n",
    "print(label_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img, label):\n",
    "        self.img = torch.Tensor(img.copy())\n",
    "        self.label = torch.Tensor(label)\n",
    "        self.len = len(img)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.img[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(images, (label_df.midquote_target).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT THE IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def add_time_units(original_starttime, unit, units_to_add):\n",
    "    # Define the conversion of each unit to seconds\n",
    "    unit_conversion = {\n",
    "        '60S': 60,         # 60 seconds\n",
    "        '30S': 30,         # 30 seconds\n",
    "        '10S': 10,         # 10 second\n",
    "        '5S': 5,         # 5 second\n",
    "        '1S': 1,         # 1 second\n",
    "        '100L': 0.1,     # 0.1 seconds\n",
    "        '10L': 0.01,     # 0.01 seconds\n",
    "        '1L': 0.001,     # 0.001 seconds\n",
    "        '100U': 0.001    # 0.001 seconds\n",
    "    }\n",
    "    # Calculate the total seconds to add\n",
    "    total_seconds_to_add = units_to_add * unit_conversion[unit]\n",
    "    # Add the time to the original datetime\n",
    "    return original_starttime + timedelta(seconds=total_seconds_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT THE IMAGE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")  \n",
    "# Directory where the image arrays are saved\n",
    "save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"  # Change this to your desired save directory\n",
    "\n",
    "#make the  new_datetime_start this format: 2023-03-21_09-30-07.000000+0000\n",
    "new_datetime_start = add_time_units(original_starttime, time_interval, 78).strftime('%Y-%m-%d_%H-%M-%S.%f%z')\n",
    "print(new_datetime_start)\n",
    "# Load the image array {asset}_depth{depth}_interval{time_interval}_window{window}_{idx}.npz\n",
    "filename = f\"depth{depth}_time{time_interval}_window{window}_date2023-03-21/{asset_list[0]}_depth{depth}_interval{time_interval}_window{window}_date2023-03-21_{new_datetime_start}.npz\"\n",
    "\n",
    "load_path = os.path.join(save_dir, filename)\n",
    "img_array = np.load(load_path)['arr_0']\n",
    "print(filename)\n",
    "# Display the image\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.imshow(img_array, cmap='gray')\n",
    "\n",
    "# display the dimension of the image\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE THE FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE THE FOLDERS\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_all_in_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "# Define your folder paths\n",
    "folder1 = f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/Returns/depth{depth}_time{time_interval}_window{window}\"\n",
    "folder2 = f\"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth{depth}_time{time_interval}_window{window}\"\n",
    "\n",
    "# Delete all elements in both folders\n",
    "delete_all_in_folder(folder1)\n",
    "delete_all_in_folder(folder2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLAYING AROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory where the image arrays are saved\n",
    "save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"  # Change this to your desired save directory\n",
    "\n",
    "# Load the image array {asset}_depth{depth}_interval{time_interval}_window{window}_{idx}.npz\n",
    "filename = f\"depth{depth}_time{time_interval}_window{window}/{asset[0]}_depth{depth}_interval{time_interval}_window{window}_2023-03-21_09-30-03.600000+0000.npz\"\n",
    "load_path = os.path.join(save_dir, filename)\n",
    "img_array = np.load(load_path)['arr_0']\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img_array, cmap='gray')\n",
    "\n",
    "# display the dimensions of the image\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_npz_files_in_batches(directory, batch_size=10000000000):\n",
    "    df_list = []\n",
    "    batch_count = 0\n",
    "\n",
    "    # Get all .npz files and sort them\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.npz')]\n",
    "    sorted_files = sorted(all_files)\n",
    "\n",
    "    # Iterate through the sorted list of files\n",
    "    for filename in sorted_files:\n",
    "        #print(f\"Processing file: {filename}\")  # Print the filename being processed\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        data = np.load(file_path)['arr_0']\n",
    "        df_list.append(data)\n",
    "        batch_count += 1\n",
    "\n",
    "        if batch_count >= batch_size:\n",
    "            yield df_list\n",
    "            df_list = []\n",
    "            batch_count = 0\n",
    "\n",
    "    if df_list:  # Yield remaining files in the last batch\n",
    "        yield df_list\n",
    "\n",
    "# Usage\n",
    "directory = '/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth10_time10L_window5'\n",
    "for df_array in load_npz_files_in_batches(directory):\n",
    "    # Process each batch here\n",
    "    print(f\"Batch size: {len(df_array)}\")  # Print the size of each batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the imageA\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(df_array[59], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the predictors for the model train the model on this!\n",
    "i = 3\n",
    "print(df_array[i])\n",
    "print(returns_df['midquote'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEST ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Helper function to concatenate orderbooks\n",
    "def concatenate_orderbooks(asset_list, date, level, directory):\n",
    "    orderbook_df_list = []\n",
    "    for asset in asset_list:\n",
    "        file_path = os.path.join(directory, f'{asset}_{date}_orderbook_{level}.feather')\n",
    "        orderbook_df_list.append(pd.read_feather(file_path))\n",
    "    orderbook = pd.concat(orderbook_df_list)\n",
    "    return orderbook\n",
    "\n",
    "def round_up_timestamp(ts, freq):\n",
    "    \"\"\"Round up a timestamp based on a given frequency\"\"\"\n",
    "    return (ts + pd.Timedelta(freq) - pd.Timedelta('1ns')).floor(freq)\n",
    "\n",
    "def split_by_seconds_optimized(df, freq, window):\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    # Round up the timestamps\n",
    "    df['ts'] = df['ts'].apply(lambda x: round_up_timestamp(x, freq))\n",
    "    df.set_index('ts', inplace=True)\n",
    "    if 'asset' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have an 'asset' column\")\n",
    "    asset_results = {}\n",
    "    for asset, asset_df in df.groupby('asset'):\n",
    "        # Resample the dataframe\n",
    "        resampled_df = asset_df.resample(freq).last()\n",
    "        \n",
    "        # Forward-fill any NaN values\n",
    "        resampled_df.ffill(inplace=True)\n",
    "        \n",
    "        # If the first row(s) are still NaN, drop them\n",
    "        resampled_df.dropna(inplace=True)\n",
    "        \n",
    "        interval_dataframes = resampled_df.groupby(resampled_df.index).tail(1)\n",
    "        cols_to_drop = [\"type\", \"order_id\", \"m_size\", \"m_price\", \"direction\", \"spread\", \"hidden_volume\", \"volume\", \"midquote\", \"asset\"]\n",
    "        interval_dataframes.drop(columns=cols_to_drop, inplace=True)\n",
    "        interval_dataframes = interval_dataframes.T\n",
    "        rolling_windows = [None] * (len(interval_dataframes.columns) - (window - 1))\n",
    "        for start_col in range(len(interval_dataframes.columns) - (window - 1)):\n",
    "            end_col = start_col + window\n",
    "            window_df = interval_dataframes.iloc[:, start_col:end_col]\n",
    "            rolling_windows[start_col] = window_df\n",
    "        asset_results[asset] = rolling_windows\n",
    "    return asset_results\n",
    "\n",
    "\n",
    "# Function to display the first x*4 rows of each DataFrame for each asset in the results\n",
    "# ... [Previous helper functions remain unchanged]\n",
    "\n",
    "# Function to truncate the results based on x\n",
    "def truncate_results(asset_results, depth):\n",
    "    truncated_results = {}\n",
    "    num_rows = depth * 4  # Convert depth to number of rows to keep\n",
    "    \n",
    "    for asset, windows in asset_results.items():\n",
    "        truncated_windows = [window.head(num_rows) for window in windows]\n",
    "        truncated_results[asset] = truncated_windows\n",
    "    \n",
    "    return truncated_results\n",
    "\n",
    "\n",
    "# Main function to process orderbooks\n",
    "def process_orderbooks(asset_list, date, level, freq, window, start_time, end_time, directory, depth):\n",
    "    orderbook = concatenate_orderbooks(asset_list, date, level, directory)\n",
    "    orderbook = orderbook.loc[orderbook['ts'].between(start_time, end_time)].reset_index(drop=True)\n",
    "    asset_results = split_by_seconds_optimized(orderbook, freq, window)\n",
    "    asset_results = truncate_results(asset_results, depth)\n",
    "    return asset_results\n",
    "\n",
    "# USE:\n",
    "asset = ['TSLA']\n",
    "date = \"2023-03-21\"\n",
    "level = 50\n",
    "directory = r\"/Users/jensknudsen/Desktop/LOBSTER_DATA/Data\"\n",
    "start_time = \"2023-03-21 09:30:00.000000+00:00\"\n",
    "end_time = \"2023-03-21 09:31:00.000000+00:00\"\n",
    "#Dinamic parameters:\n",
    "depth = 50 # also 10, 30, 50\n",
    "time_interval = '1L' #: 1S=1sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 10 # also 3, 5, 10\n",
    "\n",
    "results = process_orderbooks(asset, date, level, time_interval, window, start_time, end_time, directory, depth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the a copy of the data results to be asset_results_10ms\n",
    "asset_results_10ms = results.copy()\n",
    "\n",
    "# NEW IMPROVE CODE\n",
    "from helper_functions import scale_dataframe\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "\n",
    "def scale_dataframes_concurrently(dfs):\n",
    "    # Using ProcessPoolExecutor to parallelize the scaling\n",
    "    # Using os.cpu_count() to get the number of available CPU cores\n",
    "    num_workers = os.cpu_count() - 1  # Using all cores minus one\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        return list(executor.map(scale_dataframe, dfs))\n",
    "\n",
    "def scale_asset_dataframes(asset_data):\n",
    "    for asset, dfs in asset_data.items():\n",
    "        # Use the concurrent scaling function\n",
    "        asset_data[asset] = scale_dataframes_concurrently(dfs)\n",
    "    return asset_data\n",
    "\n",
    "asset_results_10ms = scale_asset_dataframes(asset_results_10ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def detect_missing_timestamps(asset_results):\n",
    "    missing_timestamps_info = {}\n",
    "\n",
    "    for asset_name, dataframes_list in asset_results.items():\n",
    "        missing_timestamps_dataframes = []\n",
    "\n",
    "        for df_index, df in enumerate(dataframes_list):\n",
    "            # Convert column names to datetime\n",
    "            try:\n",
    "                timestamps = pd.to_datetime(df.columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting timestamps for {asset_name} at index {df_index}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate differences between consecutive timestamps\n",
    "            time_diffs = timestamps.to_series().diff()\n",
    "\n",
    "            # Check for missing intervals (greater than 0.01 seconds)\n",
    "            if any(time_diffs > pd.Timedelta(seconds=0.01)):\n",
    "                missing_timestamps_dataframes.append(df_index)\n",
    "\n",
    "        if missing_timestamps_dataframes:\n",
    "            missing_timestamps_info[asset_name] = missing_timestamps_dataframes\n",
    "\n",
    "    return missing_timestamps_info\n",
    "\n",
    "# Example usage\n",
    "missing_dfs_info = detect_missing_timestamps(asset_results_10ms)\n",
    "print(\"Dataframes with missing timestamps:\", missing_dfs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE ONE THAT WORKS\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import numba\n",
    "#\n",
    "#@numba.jit(nopython=True)\n",
    "#def scale_values(values, min_val, max_val):\n",
    "#    return (values - min_val) / (max_val - min_val)\n",
    "#\n",
    "#def scale_rowwise(df, rows):\n",
    "#    values = df.loc[rows].values\n",
    "#    min_val = values.min()\n",
    "#    max_val = values.max()\n",
    "#    scaled_values = scale_values(values, min_val, max_val)\n",
    "#    df.loc[rows] = scaled_values\n",
    "#    return df\n",
    "#\n",
    "#def scale_dataframe(dataframe):\n",
    "#    # Identify rows\n",
    "#    price_rows = dataframe.index[dataframe.index.str.contains('price')]\n",
    "#    size_rows = dataframe.index[dataframe.index.str.contains('size')]\n",
    "#\n",
    "#    # Scale rows separately\n",
    "#    dataframe = scale_rowwise(dataframe, price_rows)\n",
    "#    dataframe = scale_rowwise(dataframe, size_rows)\n",
    "#    \n",
    "#    return dataframe\n",
    "#\n",
    "#def scale_asset_dataframes(asset_data):\n",
    "#    for asset, dfs in asset_data.items():\n",
    "#        scaled_dfs = [scale_dataframe(df.copy()) for df in dfs]\n",
    "#        asset_data[asset] = scaled_dfs\n",
    "#    return asset_data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asset_results_10ms['TSLA'][0].loc[asset_results_10ms['TSLA'][0].index.str.contains('bid_price')]\n",
    "#asset_results_10ms['TSLA'][0].loc[asset_results_10ms['TSLA'][0].index.str.contains('ask_size')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##delete every element in this folder: /Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays:\n",
    "#import os, shutil\n",
    "#folder = '/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays/depth50_time1L_window10'\n",
    "#\n",
    "#for filename in os.listdir(folder):\n",
    "#    file_path = os.path.join(folder, filename)\n",
    "#    try:\n",
    "#        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "#            os.unlink(file_path)  # delete the file\n",
    "#        elif os.path.isdir(file_path):\n",
    "#            shutil.rmtree(file_path)  # delete the folder\n",
    "#    except Exception as e:\n",
    "#        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    # Image dimensions\n",
    "#def save_images_from_dataframes_fast(asset_data, save_dir, depth, time_interval, window):\n",
    "#    IMAGE_WIDTH = (len(asset_data['TSLA'][0].columns) * 20) + len(asset_data['TSLA'][0].columns)\n",
    "#    IMAGE_HEIGHT = int((len(asset_data['TSLA'][0]) / 2) * 2)\n",
    "#    # Define pixel boundaries for price values\n",
    "#    pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "#\n",
    "#    def dataframe_to_image_arrays(df):\n",
    "#        # Create an array to store images for all columns\n",
    "#\n",
    "#        all_images = np.zeros(shape=(df.shape[1], IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "#        \n",
    "#        for col_idx, column in enumerate(df.columns):\n",
    "#            mid = (col_idx * 21) + 10\n",
    "#            aggregated_sizes = np.zeros(IMAGE_HEIGHT)\n",
    "#            # Extract prices and sizes for the current column\n",
    "#            prices = df[column].iloc[::2].values\n",
    "#\n",
    "#            sizes = df[column].iloc[1::2].values\n",
    "#            price_positions = IMAGE_HEIGHT - np.digitize(prices, pixel_boundaries)\n",
    "#            price_positions = np.clip(price_positions, 0, IMAGE_HEIGHT-1)\n",
    "#            \n",
    "#            # Aggregate sizes for this pixel value\n",
    "#            np.add.at(aggregated_sizes, price_positions, sizes)\n",
    "#            \n",
    "#            # Normalize the aggregated sizes for this column\n",
    "#            max_size = np.max(aggregated_sizes)\n",
    "#            normalized_sizes = aggregated_sizes / max_size if max_size != 0 else aggregated_sizes\n",
    "#            for idx in range(0, len(prices)):\n",
    "#                price_pos = price_positions[idx]\n",
    "#\n",
    "#                all_images[col_idx, price_pos, mid] = 255\n",
    "#                \n",
    "#                # Use normalized aggregated size for this pixel value\n",
    "#                size_value = normalized_sizes[price_pos]\n",
    "#                line_length = int(10 * size_value)\n",
    "#                \n",
    "#                if 'ask_size' in df.index[2*idx + 1]:\n",
    "#                    all_images[col_idx, price_pos, mid:mid+line_length] = 255\n",
    "#                else:\n",
    "#                    all_images[col_idx, price_pos, mid-line_length:mid] = 255\n",
    "#        \n",
    "#        return all_images\n",
    "#    # Loop through each asset and each of its dataframes\n",
    "#    for asset, dfs in asset_data.items():\n",
    "#\n",
    "#        for df in dfs:\n",
    "#            all_images_array = dataframe_to_image_arrays(df)\n",
    "#            for col_idx, column in enumerate(df.columns):\n",
    "#                img_array = all_images_array[col_idx]\n",
    "#                # Incorporate the column name in the filename and removed the idx\n",
    "#                filename = f\"{asset}_{column}_depth{depth}_interval{time_interval}_window{window}.npz\"\n",
    "#                save_path = os.path.join(save_dir, filename)\n",
    "#                np.savez_compressed(save_path, img_array)\n",
    "## The function processes the entire dataframe at once and then saves images for each column afterward.\n",
    "#\n",
    "## You'll need to test this version on your own since I don't have the actual data.\n",
    "#save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"\n",
    "#\n",
    "#save_images_from_dataframes_fast(asset_results_10ms, save_dir, depth, time_interval, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLAES CODE:\n",
    "#import re\n",
    "#\n",
    "#def sanitize_filename(filename):\n",
    "#    \"\"\"Sanitize the filename by replacing spaces and special characters with underscores.\"\"\"\n",
    "#    sanitized = re.sub(r'[^\\w\\s]', '_', filename)  # Replace special characters with underscores\n",
    "#    sanitized = sanitized.replace(' ', '_')  # Replace spaces with underscores\n",
    "#    return sanitized\n",
    "#\n",
    "#def save_images_from_dataframes_sanitized(asset_data, save_dir, depth, time_interval, window):\n",
    "#    # Image dimensions\n",
    "#    IMAGE_WIDTH = (len(asset_data['TSLA'][0].columns) * 20) + len(asset_data['TSLA'][0].columns)\n",
    "#    IMAGE_HEIGHT = int((len(asset_data['TSLA'][0]) / 2) * 2)\n",
    "#\n",
    "#    # Define pixel boundaries for price values\n",
    "#    pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "#\n",
    "#    def dataframe_to_image_array(df):\n",
    "#        img = np.zeros(shape=(IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "#        \n",
    "#        for col_idx, column in enumerate(df.columns):\n",
    "#            mid = (col_idx * 21) + 10\n",
    "#            aggregated_sizes = np.zeros(IMAGE_HEIGHT)\n",
    "#\n",
    "#            # Extract prices and sizes for the current column\n",
    "#            prices = df[column].iloc[::2].values\n",
    "#            sizes = df[column].iloc[1::2].values\n",
    "#            price_positions = IMAGE_HEIGHT - np.digitize(prices, pixel_boundaries)\n",
    "#            price_positions = np.clip(price_positions, 0, IMAGE_HEIGHT-1)\n",
    "#            \n",
    "#            # Aggregate sizes for this pixel value\n",
    "#            np.add.at(aggregated_sizes, price_positions, sizes)\n",
    "#            \n",
    "#            # Normalize the aggregated sizes for this column\n",
    "#            max_size = np.max(aggregated_sizes)\n",
    "#            normalized_sizes = aggregated_sizes / max_size if max_size != 0 else aggregated_sizes\n",
    "#\n",
    "#            for idx in range(0, len(prices)):\n",
    "#                price_pos = price_positions[idx]\n",
    "#                img[price_pos, mid] = 255\n",
    "#                \n",
    "#                # Use normalized aggregated size for this pixel value\n",
    "#                size_value = normalized_sizes[price_pos]\n",
    "#                line_length = int(10 * size_value)\n",
    "#                \n",
    "#                if 'ask_size' in df.index[2*idx + 1]:\n",
    "#                    img[price_pos, mid:mid+line_length] = 255\n",
    "#                else:\n",
    "#                    img[price_pos, mid-line_length:mid] = 255\n",
    "#                \n",
    "#        return img\n",
    "#\n",
    "#    # Loop through each asset and each of its dataframes\n",
    "#    for asset, dfs in asset_data.items():\n",
    "#        for df in dfs:\n",
    "#            img_array = dataframe_to_image_array(df)\n",
    "#            first_column_name = str(df.columns[0]).replace(' ', '_')  # Convert to string and replace spaces\n",
    "#            filename = f\"{asset}_{first_column_name}_depth{depth}_interval{time_interval}_window{window}.npz\"\n",
    "#            sanitized_filename = sanitize_filename(filename)\n",
    "#            save_path = os.path.join(save_dir, sanitized_filename)\n",
    "#            np.savez_compressed(save_path, img_array)\n",
    "#\n",
    "## The filename now includes the first column of the relevant dataframe, and is sanitized to be safe for file operations.\n",
    "## You'll need to test this version on your own since I don't have the actual data.\n",
    "#\n",
    "## Since I still don't have the actual data, you'll need to test this version on your own.\n",
    "#save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"\n",
    "#save_images_from_dataframes_sanitized(asset_results_10ms, save_dir, depth, time_interval, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW IMPROVED CODE\n",
    "def save_images_from_dataframes_aligned(asset_data, save_dir, depth, time_interval, window):\n",
    "    # Image dimensions\n",
    "    IMAGE_WIDTH = (len(asset_data['TSLA'][0].columns) * 20) + len(asset_data['TSLA'][0].columns)\n",
    "    IMAGE_HEIGHT = int((len(asset_data['TSLA'][0]) / 2) * 2)\n",
    "\n",
    "    # Define pixel boundaries for price values\n",
    "    pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "\n",
    "    def dataframe_to_image_array(df):\n",
    "        img = np.zeros(shape=(IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "        \n",
    "        for col_idx, column in enumerate(df.columns):\n",
    "            mid = (col_idx * 21) + 10\n",
    "            aggregated_sizes = np.zeros(IMAGE_HEIGHT)\n",
    "\n",
    "            # Extract prices and sizes for the current column\n",
    "            prices = df[column].iloc[::2].values\n",
    "            sizes = df[column].iloc[1::2].values\n",
    "            price_positions = IMAGE_HEIGHT - np.digitize(prices, pixel_boundaries)\n",
    "            price_positions = np.clip(price_positions, 0, IMAGE_HEIGHT-1)\n",
    "            \n",
    "            # Aggregate sizes for this pixel value\n",
    "            np.add.at(aggregated_sizes, price_positions, sizes)\n",
    "            \n",
    "            # Normalize the aggregated sizes for this column\n",
    "            max_size = np.max(aggregated_sizes)\n",
    "            normalized_sizes = aggregated_sizes / max_size if max_size != 0 else aggregated_sizes\n",
    "\n",
    "            for idx in range(0, len(prices)):\n",
    "                price_pos = price_positions[idx]\n",
    "                img[price_pos, mid] = 255\n",
    "                \n",
    "                # Use normalized aggregated size for this pixel value\n",
    "                size_value = normalized_sizes[price_pos]\n",
    "                line_length = int(10 * size_value)\n",
    "                \n",
    "                if 'ask_size' in df.index[2*idx + 1]:\n",
    "                    img[price_pos, mid:mid+line_length] = 255\n",
    "                else:\n",
    "                    img[price_pos, mid-line_length:mid] = 255\n",
    "                \n",
    "        return img\n",
    "\n",
    "    # Loop through each asset and each of its dataframes\n",
    "    for asset, dfs in asset_data.items():\n",
    "        for idx, df in enumerate(dfs):\n",
    "            img_array = dataframe_to_image_array(df)\n",
    "            filename = f\"{asset}_depth{depth}_interval{time_interval}_window{window}_{idx}.npz\"\n",
    "            save_path = os.path.join(save_dir, filename)\n",
    "            np.savez_compressed(save_path, img_array)\n",
    "\n",
    "# This function attempts to mirror the original function's behavior more closely while using \n",
    "# efficient NumPy operations to improve speed. Testing this with the data is essential.\n",
    "\n",
    "# Since I still don't have the actual data, you'll need to test this version on your own.\n",
    "save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"\n",
    "save_images_from_dataframes_aligned(asset_results_10ms, save_dir, depth, time_interval, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD GOOD CODE\n",
    "#import numpy as np\n",
    "#import os\n",
    "#\n",
    "#def save_images_from_dataframes(asset_data, save_dir, depth, time_interval, window):\n",
    "#    # Image dimensions\n",
    "#    IMAGE_WIDTH = (len(asset_data['TSLA'][0].columns) * 20) + len(asset_data['TSLA'][0].columns)\n",
    "#    IMAGE_HEIGHT = int((len(asset_data['TSLA'][0]) / 2) * 2)\n",
    "#\n",
    "#    # Define pixel boundaries for price values\n",
    "#    pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "#\n",
    "#    def dataframe_to_image_array(df):\n",
    "#        img = np.zeros(shape=(IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "#        for col_idx, column in enumerate(df.columns):\n",
    "#            mid = (col_idx * 21) + 10\n",
    "#            aggregated_sizes = {}\n",
    "#            for idx in range(0, len(df.index), 2):  # Step by 2 because of price-size pairs\n",
    "#                price_label = df.index[idx]\n",
    "#                size_label = df.index[idx + 1]\n",
    "#                \n",
    "#                price_value = df[column][price_label]\n",
    "#                price_pos = IMAGE_HEIGHT - np.digitize(price_value, pixel_boundaries)\n",
    "#                price_pos = max(0, min(IMAGE_HEIGHT-1, price_pos))\n",
    "#                \n",
    "#                # Aggregate sizes for this pixel value\n",
    "#                size_value = df[column][size_label]\n",
    "#                aggregated_sizes[price_pos] = aggregated_sizes.get(price_pos, 0) + size_value\n",
    "#\n",
    "#            # Normalize the aggregated sizes for this column\n",
    "#            max_size = max(aggregated_sizes.values())\n",
    "#            for key in aggregated_sizes:\n",
    "#                aggregated_sizes[key] /= max_size\n",
    "#\n",
    "#            for idx in range(0, len(df.index), 2):\n",
    "#                price_label = df.index[idx]\n",
    "#                size_label = df.index[idx + 1]\n",
    "#                \n",
    "#                price_value = df[column][price_label]\n",
    "#                price_pos = IMAGE_HEIGHT - np.digitize(price_value, pixel_boundaries)\n",
    "#                price_pos = max(0, min(IMAGE_HEIGHT-1, price_pos))\n",
    "#                \n",
    "#                img[price_pos, mid] = 255\n",
    "#                \n",
    "#                # Use normalized aggregated size for this pixel value\n",
    "#                size_value = aggregated_sizes[price_pos]\n",
    "#                line_length = int(10 * size_value)\n",
    "#                \n",
    "#                if 'ask_size' in size_label:\n",
    "#                    img[price_pos, mid:mid+line_length] = 255\n",
    "#                else:\n",
    "#                    img[price_pos, mid-line_length:mid] = 255\n",
    "#                \n",
    "#        return img\n",
    "#\n",
    "#    # Loop through each asset and each of its dataframes\n",
    "#    for asset, dfs in asset_data.items():\n",
    "#        for idx, df in enumerate(dfs):\n",
    "#            img_array = dataframe_to_image_array(df)\n",
    "#            filename = f\"{asset}_depth{depth}_interval{time_interval}_window{window}_{idx}.npz\"\n",
    "#            save_path = os.path.join(save_dir, filename)\n",
    "#            np.savez_compressed(save_path, img_array)\n",
    "#\n",
    "#\n",
    "#save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"\n",
    "#save_images_from_dataframes(asset_results_10ms, save_dir, depth, time_interval, window)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory where the image arrays are saved\n",
    "save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"  # Change this to your desired save directory\n",
    "\n",
    "# Load the image array {asset}_depth{depth}_interval{time_interval}_window{window}_{idx}.npz\n",
    "filename = \"TSLA_depth50_interval100L_window10_1.npz\"\n",
    "load_path = os.path.join(save_dir, filename)\n",
    "img_array = np.load(load_path)['arr_0']\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display the image\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#import os\n",
    "#\n",
    "## Directory where the image arrays are saved\n",
    "#save_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"  # Change this to your desired save directory\n",
    "#\n",
    "## Load the image array\n",
    "#filename = \"TSLA_2023_03_21_09_30_40_00_00_depth50_interval100L_window5_npz.npz\"\n",
    "#load_path = os.path.join(save_dir, filename)\n",
    "#img_array = np.load(load_path)['arr_0']\n",
    "#\n",
    "## Display the image\n",
    "#plt.figure(figsize=(20, 10))\n",
    "#plt.imshow(img_array, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "use_ramdon_split = False\n",
    "use_dataparallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "#sys.path.insert(0, '..')\n",
    "\n",
    "# if use_gpu:\n",
    "#     from utils.gpu_tools import *\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_images(directory):\n",
    "    all_images = []\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.npz')]\n",
    "    shapes = []\n",
    "    \n",
    "    for file in all_files:\n",
    "        with np.load(os.path.join(directory, file)) as data:\n",
    "            img_array = data['arr_0']\n",
    "            shapes.append(img_array.shape)\n",
    "            all_images.append(img_array)\n",
    "            \n",
    "    unique_shapes = set(shapes)\n",
    "    if len(unique_shapes) > 1:\n",
    "        print(f\"Found inconsistent shapes: {unique_shapes}\")\n",
    "    \n",
    "    # Depending on your needs, you can return a list of arrays instead of a single numpy array\n",
    "    return all_images\n",
    "\n",
    "# Load the images\n",
    "image_dir = \"/Users/jensknudsen/Desktop/LOBSTER_DATA/PROJECT/container_for_arrays\"\n",
    "images = load_saved_images(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS IS THE MAIN PICTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame(asset_results_10ms['TSLA'][100])\n",
    "\n",
    "# Check if the DataFrame needs transposing\n",
    "if 'ask_price_1' in df.columns:\n",
    "    df = df.transpose()\n",
    "\n",
    "# Image dimensions\n",
    "IMAGE_WIDTH = (len(df.columns) * 20) + len(df.columns)\n",
    "IMAGE_HEIGHT = int((len(df) / 2) * 2)\n",
    "\n",
    "img = np.zeros(shape=(IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)\n",
    "\n",
    "# Define pixel boundaries for price values\n",
    "pixel_boundaries = np.linspace(0, 1, IMAGE_HEIGHT+1)\n",
    "\n",
    "# Plotting\n",
    "for col_idx, column in enumerate(df.columns):\n",
    "    mid = (col_idx * 21) + 10\n",
    "    aggregated_sizes = {}\n",
    "\n",
    "    # Step 1: Aggregate the sizes that fall into the same pixel\n",
    "    for idx in range(0, len(df.index), 2):  # Step by 2 because of price-size pairs\n",
    "        price_label = df.index[idx]\n",
    "        size_label = df.index[idx + 1]\n",
    "        \n",
    "        price_value = df[column][price_label]\n",
    "        \n",
    "        # Determine the corresponding pixel for the price value (inverted for correct placement)\n",
    "        price_pos = IMAGE_HEIGHT - np.digitize(price_value, pixel_boundaries)\n",
    "        price_pos = max(0, min(IMAGE_HEIGHT-1, price_pos))\n",
    "        \n",
    "        # Aggregate sizes for this pixel value\n",
    "        size_value = df[column][size_label]\n",
    "        aggregated_sizes[price_pos] = aggregated_sizes.get(price_pos, 0) + size_value\n",
    "\n",
    "    # Step 2: Normalize the aggregated sizes for this column\n",
    "    max_size = max(aggregated_sizes.values())\n",
    "    for key in aggregated_sizes:\n",
    "        aggregated_sizes[key] /= max_size\n",
    "\n",
    "    for idx in range(0, len(df.index), 2):\n",
    "        price_label = df.index[idx]\n",
    "        size_label = df.index[idx + 1]\n",
    "        \n",
    "        price_value = df[column][price_label]\n",
    "        \n",
    "        # Determine the corresponding pixel for the price value (inverted for correct placement)\n",
    "        price_pos = IMAGE_HEIGHT - np.digitize(price_value, pixel_boundaries)\n",
    "        price_pos = max(0, min(IMAGE_HEIGHT-1, price_pos))\n",
    "        img[price_pos, mid] = 255\n",
    "        \n",
    "        # Use normalized aggregated size for this pixel value\n",
    "        size_value = aggregated_sizes[price_pos]\n",
    "        \n",
    "        line_length = int(10 * size_value)\n",
    "        \n",
    "        if 'ask_size' in size_label:\n",
    "            img[price_pos, mid:mid+line_length] = 255\n",
    "        else:\n",
    "            img[price_pos, mid-line_length:mid] = 255\n",
    "\n",
    "# Displaying the image\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(img, cmap='gray', aspect='auto')\n",
    "plt.xlabel('Timestamps')\n",
    "plt.ylabel('Scaled Value')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PASTE ZIP FILES INTO A FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py7zr\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Set the directory containing the zip files\n",
    "source_directory = '/Users/jensknudsen/Desktop/zip_folder'\n",
    "# Set the directory where you want to extract the files\n",
    "destination_directory = '/Users/jensknudsen/Desktop/LOBSTER_DATA/Data'\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "# Iterate over each file in the source directory\n",
    "for filename in os.listdir(source_directory):\n",
    "    if filename.endswith('.7z'):\n",
    "        # Full path to the .7z file\n",
    "        file_path = os.path.join(source_directory, filename)\n",
    "        print(f\"Processing {filename}...\")  # Debugging: print the file being processed\n",
    "        try:\n",
    "            # Open the .7z file\n",
    "            with py7zr.SevenZipFile(file_path, mode='r') as archive:\n",
    "                # Extract all files into a temporary directory within the loop\n",
    "                temp_dir = os.path.join(source_directory, \"temp_extract\")\n",
    "                archive.extractall(path=temp_dir)\n",
    "                \n",
    "                # Move only CSV files to the destination directory\n",
    "                for root, dirs, files in os.walk(temp_dir):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.csv'):\n",
    "                            shutil.move(os.path.join(root, file), os.path.join(destination_directory, file))\n",
    "                \n",
    "                # Cleanup the temporary extraction directory\n",
    "                shutil.rmtree(temp_dir)\n",
    "                \n",
    "            print(f\"Extracted CSV files from {filename} successfully.\")\n",
    "        except py7zr.SevenZipFileError as e:\n",
    "            print(f\"Failed to extract {filename}. The file may be corrupt or not a .7z file. Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with {filename}: {e}\")\n",
    "\n",
    "print(\"Done processing all .7z files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devide tickers into branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    'AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AFL', 'AIG', 'AJG', 'AMAT', 'AMD', 'AMGN', 'AMT', 'AMX', 'AMZN', 'ANET', 'AON', 'APD', 'APH', 'APO', 'ARM', 'ASML', 'AVGO', 'AXP', 'AZN', 'AZO', 'BA', 'BABA', 'BAC', 'BBVA', 'BDX', 'BHP', 'BKNG', 'BLK', 'BMO', 'BMY', 'BN', 'BNS', 'BP', 'BRK.B', 'BSX', 'BTI', 'BUD', 'BX', 'C', 'CAT', 'CB', 'CDNS', 'CHTR', 'CI', 'CL', 'CMCSA', 'CME', 'CMG', 'CNI', 'CNQ', 'COF', 'COP', 'COST', 'CP', 'CRH', 'CRM', 'CRWD', 'CSCO', 'CSX', 'CTAS', 'CVS', 'CVX', 'DE', 'DELL', 'DEO', 'DHI', 'DHR', 'DIS', 'DUK', 'E', 'ECL', 'EL', 'ELV', 'EMR', 'ENB', 'EOG', 'EPD', 'EQIX', 'EQNR', 'ET', 'ETN', 'EW', 'FCX', 'FDX', 'FI', 'FMX', 'FTNT', 'GD', 'GE', 'GILD', 'GM', 'GOOGL', 'GS', 'GSK', 'HCA', 'HD', 'HDB', 'HLT', 'HMC', 'HON', 'HSBC', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'INTU', 'ISRG', 'ITUB', 'ITW', 'JNJ', 'JPM', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY', 'LMT', 'LOW', 'LRCX', 'LULU', 'MA', 'MAR', 'MCD', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MELI', 'MET', 'META', 'MMC', 'MMM', 'MNST', 'MO', 'MPC', 'MRK', 'MRVL', 'MS', 'MSCI', 'MSFT', 'MSI', 'MU', 'MUFG', 'NEE', 'NFLX', 'NGG', 'NKE', 'NOC', 'NOW', 'NSC', 'NTES', 'NVDA', 'NVO', 'NVS', 'NXPI', 'ORCL', 'ORLY', 'OXY', 'PANW', 'PBR', 'PBR.A', 'PCAR', 'PDD', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD', 'PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROP', 'ROST', 'RSG', 'RTX', 'RY', 'SAN', 'SAP', 'SBUX', 'SCCO', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SMFG', 'SNOW', 'SNPS', 'SNY', 'SO', 'SONY', 'SPGI', 'STLA', 'SYK', 'T', 'TD', 'TDG', 'TEAM', 'TFC', 'TGT', 'TJX', 'TM', 'TMO', 'TMUS', 'TRI', 'TRV', 'TSLA', 'TSM', 'TT', 'TTE', 'TXN', 'UBER', 'UBS', 'UL', 'UNH', 'UNP', 'UPS', 'USB', 'V', 'VALE', 'VLO', 'VRTX', 'VZ', 'WDAY', 'WELL', 'WFC', 'WM', 'WMT', 'XOM', 'ZTS'\n",
    "]\n",
    "\n",
    "branches = {\n",
    "    'Technology': ['AAPL', 'ADBE', 'ADI', 'ADSK', 'AMAT', 'AMD', 'AMZN', 'ASML', 'AVGO', 'CRM', 'CDNS', 'CSCO', 'CTAS', 'EA', 'IBM', 'INTC', 'INTU', 'MSFT', 'MU', 'NVDA', 'ORCL', 'PANW', 'SHOP', 'SNPS', 'TSM', 'TXN'],\n",
    "    'Healthcare': ['ABBV', 'ABT', 'AON', 'AZN', 'BDX', 'BIIB', 'BMY', 'BSX', 'CI', 'CVS', 'DHR', 'GILD', 'HCA', 'JNJ', 'LLY', 'MDT', 'MRK', 'PFE', 'REGN', 'SNY', 'SYK', 'UNH', 'VRTX'],\n",
    "    'Financial Services': ['AIG', 'AXP', 'BAC', 'BKNG', 'BLK', 'BRK.B', 'C', 'CME', 'COF', 'GS', 'ICE', 'JPM', 'MA', 'MCO', 'MMC', 'MS', 'PNC', 'SCHW', 'SPGI', 'TFC', 'V', 'WFC'],\n",
    "    'Consumer Goods': ['EL', 'GM', 'HD', 'KO', 'LULU', 'MAR', 'MDLZ', 'NKE', 'PG', 'PM', 'PEP', 'RACE', 'RIO', 'RL', 'SBUX', 'TGT', 'TM', 'UL', 'UN', 'VLO', 'WMT'],\n",
    "    'Energy': ['APA', 'BP', 'COP', 'CVX', 'E', 'ENB', 'EOG', 'EQNR', 'EPD', 'HES', 'KMI', 'MRO', 'MPC', 'OXY', 'PBR', 'PSX', 'RDS.A', 'RDS.B', 'SU', 'VLO', 'XOM'],\n",
    "    'Industrial': ['BA', 'CAT', 'DE', 'EMR', 'GE', 'HON', 'LMT', 'MMM', 'NOC', 'RTX', 'UPS'],\n",
    "    'Retail': ['AMZN', 'BABA', 'COST', 'JD'],\n",
    "    'Telecommunication': ['CHTR', 'CMCSA', 'TMUS', 'T', 'VZ'],\n",
    "    'Automotive': ['F', 'GM', 'HMC', 'TSLA', 'TM'],\n",
    "    'Entertainment': ['DIS', 'NFLX', 'SNE'],\n",
    "    'Agriculture': ['ADM', 'BHP', 'CNI', 'CP', 'FMC', 'MOS', 'NTR'],\n",
    "    'Utilities': ['AEP', 'DUK', 'EXC', 'NEE', 'SO'],\n",
    "    'Pharmaceuticals': ['ABBV', 'AZN', 'BMY', 'GILD', 'JNJ', 'MRK', 'PFE', 'SNY'],\n",
    "    'Insurance': ['AIG', 'AON', 'BRK.B', 'MET', 'PRU', 'TRV']\n",
    "}\n",
    "\n",
    "ticker_to_branch = {}\n",
    "for branch, companies in branches.items():\n",
    "    for company in companies:\n",
    "        ticker_to_branch[company] = branch\n",
    "\n",
    "print(ticker_to_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_branch['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN (FROM COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "df_returns = pd.DataFrame()\n",
    "asset_list = ['AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AEP', 'ALGN', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'ANSS', 'ASML', 'AVGO', 'AZN', 'BIIB', 'BKNG', 'BKR', 'CDNS', 'CEG', 'CHTR', 'CMCSA', 'COST', 'CPRT', 'CRWD', 'CSCO', 'CSGP', 'CSX', 'CTAS', 'CTSH', 'DDOG', 'DLTR', 'DXCM', 'EA', 'EBAY', 'ENPH', 'EXC', 'FANG', 'FAST', 'FTNT', 'GEHC', 'GFS', 'GILD', 'GOOGL', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'JD', 'KDP', 'KHC', 'KLAC', 'LCID', 'LRCX', 'LULU', 'MAR', 'MDLZ', 'MELI', 'META', 'MNST', 'MRNA', 'MRVL', 'MSFT', 'MU', 'NFLX', 'NVDA', 'NXPI', 'ODFL', 'ON', 'ORLY', 'PANW', 'PAYX', 'PCAR', 'PDD', 'PEP', 'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SGEN', 'SIRI', 'SNPS', 'TEAM', 'TMUS', 'TSLA', 'TXN', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZM', 'ZS']\n",
    "# 'AAPL', 'ABNB', 'ADBE', 'ADI', 'ADP', 'ADSK', 'AEP', 'ALGN', 'AMAT', 'AMD', 'AMGN', 'AMZN', 'ANSS', 'ASML', 'AVGO', 'AZN', 'BIIB', 'BKNG', 'BKR', 'CDNS', 'CEG', 'CHTR', 'CMCSA', 'COST', 'CPRT', 'CRWD', 'CSCO', 'CSGP', 'CSX', 'CTAS', 'CTSH', 'DDOG', 'DLTR', 'DXCM', 'EA', 'EBAY', 'ENPH', 'EXC', 'FANG', 'FAST', 'FTNT', 'GEHC', 'GFS', 'GILD', 'GOOGL', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'JD', 'KDP', 'KHC', 'KLAC', 'LCID', 'LRCX', 'LULU', 'MAR', 'MDLZ', 'MELI', 'META', 'MNST', 'MRNA', 'MRVL', 'MSFT', 'MU', 'NFLX', 'NVDA', 'NXPI', 'ODFL', 'ON', 'ORLY', 'PANW', 'PAYX', 'PCAR', 'PDD', 'PEP', 'PYPL', 'QCOM', 'REGN', 'ROST', 'SBUX', 'SGEN', 'SIRI', 'SNPS', 'TEAM', 'TMUS', 'TSLA', 'TXN', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZM', 'ZS' #'GOOG','MCHP','FI'\n",
    "\n",
    "original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")\n",
    "\n",
    "level = 50\n",
    "prediction_ahead = 5\n",
    "depth = 5 # also 10, 30, 50\n",
    "time_interval = '1S' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n",
    "window = 5 # also 3, 5, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/DataLOB_array/combined_depth5_time1S_window5.pkl'\n",
    "df = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)\n",
    "# Convert each NumPy array in the 'Arrays' column to a PyTorch tensor\n",
    "df['tensors'] = df['Arrays'].apply(lambda x: torch.from_numpy(x))\n",
    "# Assuming your DataFrame has a column 'tensors' with the tensors\n",
    "df['tensors'] = df['tensors'].apply(lambda x: (x.float() / 255).to(torch.float32))\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Apply .to(device='cuda') to each tensor in the column\n",
    "    df['tensors'] = df['tensors'].apply(lambda x: x.to(device='cuda'))\n",
    "    print(\"CUDA is avaibable and is now applied to each tensor\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Tensors will remain on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_dataframe_by_ticker(df, time_column='Time', train_ratio=0.6, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training, validation, and test sets based on ticker.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    time_column (str): The name of the column containing time data.\n",
    "    train_ratio (float): The proportion of data to be used for training.\n",
    "    val_ratio (float): The proportion of data to be used for validation.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Training set.\n",
    "    DataFrame: Validation set.\n",
    "    DataFrame: Test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Time column to datetime for proper sorting\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "    # Define a function to split the data\n",
    "    def split_data(group):\n",
    "        group = group.sort_values(by=time_column)\n",
    "        idx_train = int(len(group) * train_ratio)\n",
    "        idx_val = int(len(group) * (train_ratio + val_ratio))\n",
    "        return group.iloc[:idx_train], group.iloc[idx_train:idx_val], group.iloc[idx_val:]\n",
    "\n",
    "    # Apply the function and get splits\n",
    "    splits = df.groupby('Ticker').apply(lambda g: split_data(g))\n",
    "\n",
    "    # Extract splits into separate DataFrames using list comprehension\n",
    "    train_df = pd.concat([s[0] for s in splits])\n",
    "    val_df = pd.concat([s[1] for s in splits])\n",
    "    test_df = pd.concat([s[2] for s in splits])\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Usage Example:\n",
    "train_df, val_df, test_df = split_dataframe_by_ticker(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Custom Dataset Class\n",
    "class StockDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading stock tensor data\"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.dataframe.iloc[idx]['tensors']\n",
    "        label = self.dataframe.iloc[idx]['midquote_target']\n",
    "        features = features.unsqueeze(0)  # Ensure there's a channel dimension\n",
    "        return features, label\n",
    "\n",
    "# CNN Model Definition\n",
    "class StockCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StockCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 10 * 26, 120)  # Adjust based on the output size\n",
    "        self.fc2 = nn.Linear(120, 2)  # For binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Assuming train_df, val_df, and test_df are already defined and split\n",
    "train_dataset = StockDataset(train_df)\n",
    "val_dataset = StockDataset(val_df)\n",
    "test_dataset = StockDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Setup for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = StockCNN().to(device)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming train_df contains your training data\n",
    "num_class_0 = (train_df['midquote_target'] == 0).sum()\n",
    "num_class_1 = (train_df['midquote_target'] == 1).sum()\n",
    "\n",
    "total_samples = len(train_df)\n",
    "weight_for_class_0 = total_samples / (2 * num_class_0)\n",
    "weight_for_class_1 = total_samples / (2 * num_class_1)\n",
    "\n",
    "# Define weights tensor\n",
    "weights = torch.tensor([weight_for_class_0, weight_for_class_1], dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the loss function with weighted class\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Training and Validation Function\n",
    "def train_one_epoch(epoch_index, num_epochs, model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch_index+1}/{num_epochs} Training\")):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    print(f\"Training - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "def validate(epoch_index, num_epochs, model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(val_loader, desc=f\"Epoch {epoch_index+1}/{num_epochs} Validation\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets.long())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    print(f\"Validation - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "# Main Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch, num_epochs, model, device, train_loader, optimizer, criterion)\n",
    "    validate(epoch, num_epochs, model, device, val_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during testing\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        predictions.extend(predicted.view(-1).cpu().numpy())\n",
    "        true_labels.extend(labels.view(-1).cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets.long())\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions)\n",
    "    recall = recall_score(all_targets, all_predictions)\n",
    "    f1 = f1_score(all_targets, all_predictions)\n",
    "    confusion_mat = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    print(f\"Test - Loss: {epoch_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "\n",
    "# After training and validation, test the model on the test set\n",
    "test(model, device, test_loader, criterion)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Re)-imag(in)ing Price Trends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key component of image- based prediction is the implicit data scaling achieved by the image representationimages put all stocks past price data on the same scale so that their recent maximum high and minimum low prices span the height of the image and all other prices (open, high, low, close, and moving average) are rescaled accordingly, and likewise for volume"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main component of our image is a concatenation of daily OHLC bars over consecutive 5, 20, or 60-day intervals (approximately weekly, monthly, and quarterly price trajectories, respectively). The width of a n-day image is thus 3n pixels. We replace prices by CRSP adjusted returns to translate the opening, closing, high and low prices into relative scales that abstract from price effects of stock splits and dividend issuance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
