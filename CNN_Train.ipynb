{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1708603324995,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"7lhPMZs25YTQ","outputId":"30651fb6-4cb1-4d01-85c4-12ac0101e1bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Feb 22 12:02:04 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708603324996,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"vO-C7PYP5aiV","outputId":"557a2690-40c7-4953-f799-3a01ee7a8c5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 54.8 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3974,"status":"ok","timestamp":1708603349536,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"aqF26wed4_AB"},"outputs":[],"source":["use_gpu = True\n","use_ramdon_split = False\n","use_dataparallel = True\n","\n","import os\n","import sys\n","sys.path.insert(0, '..')\n","\n","import time\n","import datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import math\n","import importlib\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import random_split"]},{"cell_type":"markdown","metadata":{"id":"QjNSFxQM4_AB"},"source":["# Load data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":111982,"status":"ok","timestamp":1708603463680,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"KasQbYcA_ZWZ"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","IMAGE_WIDTH = {5: 40}\n","IMAGE_HEIGHT = {5: 105}\n","\n","asset_list = ['AAPL', 'ABBV', 'ABNB', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADP', 'AFL','AIG', 'ALL', 'AMAT', 'AMD', 'AMT', 'AMZN', 'ANET', 'ASML', 'AVGO','AXP', 'AZN', 'BABA', 'BAC', 'BA', 'BBVA', 'BDX', 'BHP', 'BKNG','BLK', 'BMY', 'BNS', 'BP', 'BSX', 'BTI', 'BUD', 'BX', 'CARR','CCI', 'CDNS', 'CL', 'CMCSA', 'COF', 'COP', 'COST', 'CP', 'CRH','CRM', 'CSCO', 'CSX', 'CVS', 'CVX', 'C', 'DASH', 'DEO','DE', 'DHI', 'DIS', 'DUK', 'EL', 'EMR', 'ENB', 'EOG', 'EPD','EQNR', 'ETN', 'ET', 'EW', 'FCX', 'FTNT', 'GD', 'GE', 'GILD', 'GM','GOOGL', 'GSK', 'GWW', 'HCA', 'HDB', 'HD', 'HLT', 'HMC', 'HON','HSBC', 'HUM', 'IBM', 'IBN', 'ICE', 'INFY', 'ING', 'INTC', 'ISRG','ITW', 'JNJ', 'JPM', 'KHC', 'KKR', 'KLAC', 'KO', 'LIN', 'LLY','LMT', 'LOW', 'LRCX', 'LULU', 'MAR', 'MA', 'MCD', 'MCHP', 'MCO','MDLZ', 'MDT', 'MELI', 'META', 'MET', 'MMM', 'MNST', 'MO', 'MPC','MRK', 'MRVL', 'MSCI', 'MSFT', 'MS', 'MU', 'NEE', 'NFLX', 'NGG','NKE', 'NOW', 'NSC', 'NTES', 'NUE', 'NVDA', 'NVO', 'NVS', 'NXPI','ORCL', 'ORLY', 'OXY', 'PANW', 'PAYX', 'PBR', 'PCAR', 'PEP','PFE', 'PGR', 'PG', 'PLD', 'PM', 'PNC', 'PSA', 'PSX', 'PXD','PYPL', 'QCOM', 'RACE', 'REGN', 'RELX', 'RIO', 'ROST', 'RY', 'SAN','SAP', 'SBUX', 'SCHW', 'SHEL', 'SHOP', 'SHW', 'SLB', 'SNOW','SNPS', 'SNY', 'SO', 'SPG', 'SPOT', 'STLA', 'SYK', 'TD', 'TEAM','TFC', 'TGT', 'TMUS', 'TRI', 'TSLA', 'TSM', 'TTE', 'TXN', 'T','UBER', 'UBS', 'UL', 'UNH', 'USB', 'VALE', 'VLO', 'VRTX', 'VZ','WELL', 'WFC', 'WM', 'ZTS']\n","\n","original_starttime = datetime.fromisoformat(\"2023-03-21 09:30:00.000000+00:00\")\n","\n","level = 15\n","prediction_ahead = 4\n","depth = 5 # also 10, 30, 50\n","time_interval = '1S' #: 30S = 30sek, 10S = 10sek, 5S=5sek, 1S=1sek; 100L=0,1sek; 10L=0,01sek; 1L=0,001sek; 100U=0,001sek\n","window = 5 # also 3, 5, 10\n","\n","# import the pickle file\n","df = pd.read_pickle('/home/ucloud/UCloud_input/combined_depth5_time1S_window5.pkl')"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9685,"status":"ok","timestamp":1708603473355,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"cIlAAAfY4_AC","outputId":"a66250d6-3b8a-4baf-ef6f-fb15127070be"},"outputs":[{"name":"stdout","output_type":"stream","text":["700750\n","282002\n","282002\n","564004\n","(564004, 40, 105)\n","(564004, 11)\n"]}],"source":["# only keep the rows there the ticker column is in the asset_list:\n","df = df[df['Ticker'].isin(asset_list)]\n","\n","#sort df first by 'Time' and then by 'Ticker':\n","df = df.sort_values(by=['Time', 'Ticker'])\n","\n","training_procent = 0.6\n","\n","x = len(df)*training_procent\n","\n","def closest_value(x):\n","    return int(x + 200 - x%200)\n","\n","input = closest_value(x)\n","\n","\n","train_df = df[:int(input)]\n","test_df = df[int(1-input):]\n","\n","\n","# Count the number of 0s and 1s\n","count_0 = (train_df['midquote_target'] == 0).sum()\n","count_1 = (train_df['midquote_target'] == 1).sum()\n","\n","# Determine the smaller count to equalize the distribution\n","min_count = min(count_0, count_1)\n","\n","print(len(train_df))\n","\n","# Sample min_count rows from each group\n","df_0_sample = train_df[train_df['midquote_target'] == 0].sample(n=min_count, random_state=42)\n","df_1_sample = train_df[train_df['midquote_target'] == 1].sample(n=min_count, random_state=42)\n","\n","\n","# only select 227271 random rows from the df_0_sample\n","df_0_sample = df_0_sample.sample(n=227271, random_state=42)\n","# only select 227271 random rows from the df_1_sample\n","df_1_sample = df_1_sample.sample(n=227271, random_state=42)\n","\n","print(len(df_0_sample))\n","print(len(df_1_sample))\n","\n","# Concatenate the samples to get a balanced DataFrame\n","df_balanced = pd.concat([df_0_sample, df_1_sample])\n","\n","print(len(df_balanced))\n","\n","# Optionally, shuffle the rows if you want a mixed order\n","train_df = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","\n","sys.path.insert(0, '..')\n","\n","\n","def query_gpu(qargs=[]):\n","    qargs =['index','gpu_name', 'memory.free']+ qargs\n","    cmd = 'nvidia-smi --query-gpu={} --format=csv,noheader'.format(','.join(qargs))\n","    results = os.popen(cmd).readlines()\n","    return results\n","\n","def select_gpu(results, thres=4096):\n","    avali = []\n","    try:\n","        for i, line in enumerate(results):\n","            if int(re.findall('(.*), (.*?) MiB', line)[0][-1]) > thres:\n","                avali.append(i)\n","        return avali\n","    except:\n","        return ''\n","\n","if use_gpu:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join([ str(obj) for obj in select_gpu(query_gpu())])\n","\n","# Set batch size\n","batch_size = 10\n","\n","# Initialize an empty list to store batches\n","batched_images = []\n","\n","# Process arrays in batches\n","for i in range(0, len(train_df), batch_size):\n","    batch = train_df['Arrays'].iloc[i:i+batch_size]\n","    batched_images.append(np.stack(batch))\n","\n","# Concatenate batches to create the final array\n","images = np.concatenate(batched_images)\n","\n","label_df = train_df\n","\n","# Check the shape of the resulting array\n","print(images.shape)\n","print(label_df.shape)"]},{"cell_type":"markdown","metadata":{"id":"8W8MpnE44_AC"},"source":["# build dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1708603473356,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"osTxNFzv4_AC"},"outputs":[],"source":["class MyDataset(Dataset):\n","\n","    def __init__(self, img, label):\n","        self.img = torch.Tensor(img.copy())\n","        self.label = torch.Tensor(label)\n","        self.len = len(img)\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        return self.img[idx], self.label[idx]"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":3761,"status":"ok","timestamp":1708603477106,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"01Ch6rbx4_AD"},"outputs":[],"source":["if not use_ramdon_split:\n","    train_val_ratio = 0.7\n","    split_idx = int(images.shape[0] * 0.7)\n","    train_dataset = MyDataset(images[:split_idx], (label_df.midquote_target).values[:split_idx])\n","    val_dataset = MyDataset(images[split_idx:], (label_df.midquote_target).values[split_idx:])\n","else:\n","    dataset = MyDataset(images, (label_df.midquote_target).values)\n","    train_val_ratio = 0.7\n","    train_dataset, val_dataset = random_split(dataset, \\\n","        [int(dataset.len*train_val_ratio), dataset.len-int(dataset.len*train_val_ratio)], \\\n","        generator=torch.Generator().manual_seed(42))\n","    del dataset\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1708603477106,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"2EQ-lEKO4_AD"},"outputs":[],"source":["def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.)\n","    elif isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1708603477106,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"oC-rNyV34_AD"},"outputs":[],"source":["# Original model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 64, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","            nn.MaxPool2d((2, 1), stride=(2, 1)),\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","            nn.MaxPool2d((2, 1), stride=(2, 1)),\n","        )\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=(5, 3), stride=(3, 1), dilation=(2, 1), padding=(12, 1)),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","            nn.MaxPool2d((2, 1), stride=(2, 1)),\n","        )\n","\n","        # Dynamically calculate the size of the FC layer\n","        self._temp_size = self._get_conv_output(torch.rand(1, 1, 40, 105)).nelement()\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(p=0.5),\n","            nn.Linear(self._temp_size, 2),\n","        )\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def _get_conv_output(self, input_tensor):\n","        with torch.no_grad():\n","            output = self._forward_features(input_tensor)\n","        return output\n","\n","    def _forward_features(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        return x\n","\n","    def forward(self, x):\n","        x = self._forward_features(x)\n","        x = x.view(x.size(0), -1)  # Flatten\n","        x = self.fc1(x)\n","        # x = self.softmax(x)  # It's more common to return logits directly and use softmax combined with the loss function.\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvADcT6OMpbK"},"outputs":[],"source":["## test model\n","#import torch\n","#import torch.nn as nn\n","#import torch.nn.functional as F\n","#\n","#class Net(nn.Module):\n","#    def __init__(self):\n","#        super(Net, self).__init__()\n","#        # Adjusted the first convolutional layer\n","#        self.layer1 = nn.Sequential(\n","#            nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5)),  # Reduced stride\n","#            nn.BatchNorm2d(64),\n","#            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","#            nn.MaxPool2d((2, 2), stride=(2, 2)),\n","#        )\n","#        # Adjusted the second convolutional layer\n","#        self.layer2 = nn.Sequential(\n","#            nn.Conv2d(64, 128, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5)),  # Reduced stride\n","#            nn.BatchNorm2d(128),\n","#            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","#            nn.MaxPool2d((2, 2), stride=(2, 2)),\n","#        )\n","#        # Adjusted the third convolutional layer\n","#        self.layer3 = nn.Sequential(\n","#            nn.Conv2d(128, 256, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5)),  # Reduced stride\n","#            nn.BatchNorm2d(256),\n","#            nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","#            nn.MaxPool2d((2, 2), stride=(2, 2)),\n","#        )\n","#\n","#        # Dynamically calculate the size of the FC layer\n","#        self._temp_size = self._get_conv_output(torch.rand(1, 1, 40, 105)).nelement()\n","#\n","#        # Define the fully connected layer\n","#        self.fc1 = nn.Sequential(\n","#            nn.Dropout(p=0.5),\n","#            nn.Linear(self._temp_size, 2),\n","#        )\n","#\n","#    def _get_conv_output(self, input_tensor):\n","#        \"\"\"Dynamically computes the output size of the convolutional layers.\"\"\"\n","#        with torch.no_grad():\n","#            output = self._forward_features(input_tensor)\n","#        return output\n","#\n","#    def _forward_features(self, x):\n","#        \"\"\"Passes the input tensor through convolutional layers.\"\"\"\n","#        x = self.layer1(x)\n","#        x = self.layer2(x)\n","#        x = self.layer3(x)\n","#        return x\n","#\n","#    def forward(self, x):\n","#        \"\"\"Defines the forward pass of the model.\"\"\"\n","#        x = self._forward_features(x)\n","#        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n","#        x = self.fc1(x)\n","#        return x\n","#"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3807,"status":"ok","timestamp":1708603489155,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"_I-xAXk-4_AD"},"outputs":[],"source":["#import baseline_model\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","export_onnx = True\n","net = Net().to(device)\n","net.apply(init_weights)  # Ensure init_weights is defined elsewhere\n","\n","if export_onnx:\n","    import torch.onnx\n","    x = torch.randn([1, 1, 40, 105]).to(device)\n","    torch.onnx.export(net,               # model being run\n","                      x,                         # model input (or a tuple for multiple inputs)\n","                      \"/home/ucloud/UCloud_output/content/cnn_baseline.onnx\",   # where to save the model (can be a file or file-like object)\n","                      export_params=False,        # store the trained parameter weights inside the model file\n","                      opset_version=10,          # the ONNX version to export the model to\n","                      do_constant_folding=False,  # whether to execute constant folding for optimization\n","                      input_names = ['input_images'],   # the model's input names\n","                      output_names = ['output_prob'], # the model's output names\n","                      dynamic_axes={'input_images' : {0 : 'batch_size'},    # variable length axes\n","                                     'output_prob' : {0 : 'batch_size'}})\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708603489155,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"g18cfvPyFIBX","outputId":"a7fede1f-fa9d-469a-c611-d70bc90e69cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["layer1.0.weight : torch.Size([64, 1, 5, 3])\n","layer1.0.bias : torch.Size([64])\n","layer1.1.weight : torch.Size([64])\n","layer1.1.bias : torch.Size([64])\n","layer2.0.weight : torch.Size([128, 64, 5, 3])\n","layer2.0.bias : torch.Size([128])\n","layer2.1.weight : torch.Size([128])\n","layer2.1.bias : torch.Size([128])\n","layer3.0.weight : torch.Size([256, 128, 5, 3])\n","layer3.0.bias : torch.Size([256])\n","layer3.1.weight : torch.Size([256])\n","layer3.1.bias : torch.Size([256])\n","fc1.1.weight : torch.Size([2, 80640])\n","fc1.1.bias : torch.Size([2])\n","total_parameters : 777986\n"]}],"source":["count = 0\n","for name, parameters in net.named_parameters():\n","    print(name, ':', parameters.size())\n","    count += parameters.numel()\n","print('total_parameters : {}'.format(count))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1117,"status":"ok","timestamp":1708603490269,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"LHm2lPnB4_AD"},"outputs":[],"source":["#for images, labels in train_dataloader:\n","#    # Add a channel dimension to make it [batch_size, 1, height, width]\n","#    images = images.unsqueeze(1)  # This changes shape from [128, 40, 105] to [128, 1, 40, 105]\n","#    # Now you can forward pass this through your network\n","#    outputs = net(images.to(device))\n","#    # Rest of your training loop...\n","#    break  # This break is just to stop the loop here for demonstration"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1708603490270,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"HxwAgL244_AE"},"outputs":[],"source":["def train_loop(dataloader, net, loss_fn, optimizer):\n","\n","    running_loss = 0.0\n","    current = 0\n","    net.train()\n","\n","    with tqdm(dataloader) as t:\n","        for batch, (X, y) in enumerate(t):\n","            X = X.to(device).unsqueeze(1)\n","            X = X.to(device)\n","            y = y.to(device)\n","            print(X.shape)\n","            y_pred = net(X)\n","            loss = loss_fn(y_pred, y.long())\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss = (len(X) * loss.item() + running_loss * current) / (len(X) + current)\n","            current += len(X)\n","            t.set_postfix({'running_loss':running_loss})\n","\n","    return running_loss\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708603490270,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"e9qxjE3P4_AE"},"outputs":[],"source":["def val_loop(dataloader, net, loss_fn):\n","\n","    running_loss = 0.0\n","    current = 0\n","    net.eval()\n","\n","    with torch.no_grad():\n","        with tqdm(dataloader) as t:\n","            for batch, (X, y) in enumerate(t):\n","                X = X.to(device).unsqueeze(1)\n","                X = X.to(device)\n","                y = y.to(device)\n","                y_pred = net(X)\n","                loss = loss_fn(y_pred, y.long())\n","\n","                running_loss += loss.item()\n","                running_loss = (len(X) * running_loss + loss.item() * current) / (len(X) + current)\n","                current += len(X)\n","\n","    return running_loss"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1708603490270,"user":{"displayName":"Jens Knudsen","userId":"01197893933721683580"},"user_tz":-60},"id":"oRjostZP4_AE"},"outputs":[],"source":["if use_gpu and use_dataparallel and 'DataParallel' not in str(type(net)):\n","    net = net.to(device)\n","    net = nn.DataParallel(net)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import time\n","import torch\n","import torch.nn as nn\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision import datasets, transforms\n","\n","# Assuming other necessary imports, variable initializations, and net definition are done elsewhere\n","\n","# Initialize TensorBoard\n","tb = SummaryWriter()\n","\n","# Loss function and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n","\n","start_epoch = 0\n","min_val_loss = 1e9\n","last_min_ind = -1\n","early_stopping_epoch = 5\n","\n","base_save_path = '/home/ucloud/UCloud_output/pt/'\n","best_model_path = ''\n","\n","epochs = 100\n","for t in range(start_epoch, epochs):\n","    print(f\"Epoch {t}\\n-------------------------------\")\n","    time.sleep(0.2)  # Simulate training/validation time\n","\n","    # Training and validation loops\n","    train_loss = train_loop(train_dataloader, net, loss_fn, optimizer)\n","    val_loss = val_loop(val_dataloader, net, loss_fn)\n","\n","    # Log losses to TensorBoard\n","    tb.add_scalar(\"Loss/Train\", train_loss, t)\n","    tb.add_scalar(\"Loss/Validation\", val_loss, t)\n","\n","    # Optionally, log model weights and biases\n","    for name, weight in net.named_parameters():\n","        tb.add_histogram(f\"{name}.grad\", weight.grad, t)\n","        tb.add_histogram(name, weight, t)\n","\n","    if val_loss < min_val_loss:\n","        last_min_ind = t\n","        min_val_loss = val_loss\n","\n","        best_model_filename = f'best_model_epoch_{t}_train_{train_loss:.5f}_val_{val_loss:.5f}.pt'\n","        best_model_path = os.path.join(base_save_path, best_model_filename)\n","\n","        os.makedirs(base_save_path, exist_ok=True)\n","        torch.save(net, best_model_path)\n","\n","        print(f\"New best model saved at epoch {t} with validation loss {val_loss:.5f}\")\n","\n","    elif t - last_min_ind >= early_stopping_epoch:\n","        print(\"Early stopping triggered.\")\n","        break\n","\n","# After training, add graph visualization\n","sample_inputs = next(iter(train_dataloader))[0]\n","tb.add_graph(net, sample_inputs.to(next(net.parameters()).device))\n","\n","# Close the TensorBoard writer\n","tb.close()\n","\n","print('Done!')\n","print(f'Best epoch: {last_min_ind}, val_loss: {min_val_loss}')\n","if best_model_path:\n","    print(f'Best model saved at: {best_model_path}')\n","else:\n","    print(\"No model was saved.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
